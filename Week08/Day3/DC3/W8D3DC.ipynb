{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAvqnihV4k87MWFG0UmcNd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arquansa/PSTB-exercises/blob/main/Week08/Day3/DC3/W8D3DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Daily Challenge: Evaluating Large Language Models**#\n",
        "\n",
        "Task\n",
        "\n",
        "**1. Understanding LLM Evaluation**\n",
        "\n",
        "Explain why evaluating LLMs is more complex than traditional software.\n",
        "Identify key reasons for evaluating an LLM’s safety.\n",
        "Describe how adversarial testing contributes to LLM improvement.\n",
        "Discuss the limitations of automated evaluation metrics and how they compare to human evaluation.\n",
        "\n",
        "\n",
        "**2. Applying BLEU and ROUGE Metrics:**\n",
        "\n",
        "Calculate the BLEU score for the following example:\n",
        "\n",
        "Reference: “Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.”\n",
        "Generated: “Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.”\n",
        "Calculate the ROUGE score for the following example:\n",
        "\n",
        "Reference: “In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.”\n",
        "Generated: “To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.”\n",
        "Provide an analysis of the limitations of BLEU and ROUGE when evaluating creative or context-sensitive text.\n",
        "\n",
        "Suggest improvements or alternative methods for evaluating text generation.\n",
        "\n",
        "**3. Perplexity Analysis:**\n",
        "\n",
        "Compare the perplexity of the two language models based on the probability assigned to a word:\n",
        "\n",
        "Model A: Assigns 0.8 probability to “mitigation.”\n",
        "Model B: Assigns 0.4 probability to “mitigation.”\n",
        "Determine which model has lower perplexity and explain why.\n",
        "\n",
        "Given a language model that has a perplexity score of 100, discuss its performance implications and possible ways to improve it.\n",
        "\n",
        "**4. Human Evaluation Exercise:**\n",
        "\n",
        "Rate the fluency of this chatbot response using a Likert scale (1-5): “Apologies, but comprehend I do not. Could you rephrase your question?”\n",
        "Justify your rating.\n",
        "Propose an improved version of the response and explain why it is better.\n",
        "\n",
        "\n",
        "**5. Adversarial Testing Exercise:**\n",
        "\n",
        "Identify the potential mistake an LLM might make when answering the Prompt: “What is the capitol of France?”\n",
        "\n",
        "Expected: “Paris.”\n",
        "Suggest a method to improve robustness against such errors.\n",
        "\n",
        "Create at least three tricky prompts that could challenge an LLM’s robustness, bias detection, or factual accuracy.\n",
        "\n",
        "\n",
        "**6. Comparative Analysis of Evaluation Methods:**\n",
        "\n",
        "Choose an NLP task (e.g., machine translation, text summarization, question answering).\n",
        "Compare and contrast at least three different evaluation metrics (BLEU, ROUGE, BERTScore, Perplexity, Human Evaluation, etc.).\n",
        "Discuss which metric is most appropriate for the chosen task and why.\n"
      ],
      "metadata": {
        "id": "S3DDeGv5ySDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Understanding LLM Evaluation**"
      ],
      "metadata": {
        "id": "h0cp0qKi0JZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "•\tExplain why evaluating LLMs is more complex than traditional software.\n",
        "\n",
        "Evaluating LLMs is much more complex than assessing traditional software because of what they produce: natural language. Indeed, while evaluating traditional software just implies correctness checks, the evaluation of LLMs requires more subjective answers, more sophisticated methods, and more variety since an output can be correct or not, depending on the situation\n",
        "\n",
        "•\tIdentify key reasons for evaluating an LLM’s safety.\n",
        "\n",
        "For ethical as well as legal reasons, safety of a LLM must be assessed as  the LLMs’s model should avoid producing contents that are misleading (spreading fake mews for instance), biased, and liable to reinforce stereotypes\n",
        "\n",
        "•\tDescribe how adversarial testing contributes to LLM improvement.\n",
        "\n",
        "Adversarial testing is a technique of evaluation that prompts an LLM with all sorts of tricky or harmful questions to assess its strengths and weaknesses. That method helps finding weak spots that need reinforcement through fine-tuning.\n",
        "\n",
        "•\tDiscuss the limitations of automated evaluation metrics and how they compare to human evaluation.\n",
        "\n",
        "Though it is less costly and much quicker than human evaluation, automated evaluation metrics struggles in the assessment of aspects like fluency, coherence, factual accuracy and bias, that only human beings can efficiently assess."
      ],
      "metadata": {
        "id": "PXX3Dc5hImpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Applying BLEU and ROUGE Metrics:**"
      ],
      "metadata": {
        "id": "egYP3oLsIQRc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3019347",
        "outputId": "e9122f08-f637-4116-fd11-49b80f0e8a67"
      },
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "reference = [\"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\".split()]\n",
        "generated = \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\".split()\n",
        "\n",
        "bleu_score = sentence_bleu(reference, generated)\n",
        "print(bleu_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6911564082630298e-78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result  is very close to 0 (78 decimal places to the right of the decimal point before the significant digits start) because, by default, sentence_bleu() uses BLEU-4 (up to 4-grams) without smoothing."
      ],
      "metadata": {
        "id": "bjVSgqMzzSwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "IsJPIqVbFn7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "reference = \"In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.\"\n",
        "generated = \"To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.\"\n",
        "\n",
        "scores = scorer.score(reference, generated)\n",
        "\n",
        "print(\"ROUGE Scores:\")\n",
        "for key, value in scores.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "MHUfwjamFn50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output\n",
        "With stemming enabled, typical output looks like this:\n",
        "\n",
        "{\n",
        "  'rouge1': Score(precision=0.615, recall=0.511, fmeasure=0.558),\n",
        "  'rouge2': Score(precision=0.370, recall=0.306, fmeasure=0.335),\n",
        "  'rougeL': Score(precision=0.538, recall=0.448, fmeasure=0.488)\n",
        "}\n",
        "with light variations depending on tokenizer, stemming, and whitespace handling.\n",
        "\n",
        "ROUGE-1 and ROUGE-L are moderately strong, showing good word and structural overlap.\n",
        "\n",
        "ROUGE-2 is lower, due to more paraphrasing and fewer exact bigrams."
      ],
      "metadata": {
        "id": "HT0BDINC1Ozm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating Large Language Models (LLMs) is more complex than traditional software, especially regarding safety.\n",
        "Metrics like BLEU and ROUGE are used to assess text similarity, with examples provided.\n",
        "Limitations of BLEU and ROUGE for creative or context-sensitive text are discussed.\n",
        "Alternative evaluation methods like human assessment, BERTScore, and Moverscore are suggested.\n",
        "Future sections will cover perplexity analysis and adversarial testing exercises.\n",
        "Comparing different evaluation metrics for specific NLP tasks is also included."
      ],
      "metadata": {
        "id": "zTzoXn3zNTBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Perplexity Analysis:**"
      ],
      "metadata": {
        "id": "6GACknErJFRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Perplexity Analysis:\n",
        "\n",
        "Compare the perplexity of the two language models based on the probability assigned to a word:\n",
        "\n",
        "Model A: Assigns 0.8 probability to “mitigation.”\n",
        "Model B: Assigns 0.4 probability to “mitigation.”\n",
        "Perplexity is a measure of how well a probability model predicts a sample. It is the inverse probability of the test set, normalized by the number of words. A lower perplexity score indicates a better model.\n",
        "\n",
        "The perplexity of a word with probability  p  is  1/p .\n",
        "\n",
        "Model A Perplexity:  1/0.8=1.25\n",
        "Model B Perplexity:  1/0.4=2.5\n",
        "Conclusion: Model A has lower perplexity (1.25) than Model B (2.5). This is because Model A assigns a higher probability to the word “mitigation,” indicating that it is more confident and better at predicting this word in its context.\n",
        "\n",
        "Given a language model that has a perplexity score of 100, discuss its performance implications and possible ways to improve it.\n",
        "\n",
        "A language model with a perplexity score of 100 on a given dataset suggests that, on average, the model is as uncertain about the next word as if it were choosing uniformly from 100 possible words.\n",
        "\n",
        "Performance Implications:\n",
        "\n",
        "Lower Fluency and Coherence: A high perplexity often correlates with less fluent and coherent text generation. The model is less certain about the most likely next word, leading to potentially awkward phrasing or illogical sequences.\n",
        "Increased Risk of Errors and Hallucinations: Higher uncertainty can lead to the model generating less accurate or even fabricated information (hallucinations) because it is not strongly predicting the correct words or concepts."
      ],
      "metadata": {
        "id": "juYNk_hjJtRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Human Evaluation Exercise:**"
      ],
      "metadata": {
        "id": "0X8CFH9qKUhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Human Evaluation Exercise:\n",
        "\n",
        "Rate the fluency of this chatbot response using a Likert scale (1-5): “Apologies, but comprehend I do not. Could you rephrase your question?”\n",
        "\n",
        "Rating: 2/5\n",
        "\n",
        "Justification: The response is not fluent. The phrasing \"comprehend I do not\" is grammatically incorrect and unnatural in typical English conversation. While the meaning is understandable, the awkward sentence structure hinders fluency.\n",
        "\n",
        "Propose an improved version of the response and explain why it is better.\n",
        "\n",
        "Improved Version: \"I apologize, I don't understand. Could you please rephrase your question?\" or \"Sorry, I didn't get that. Can you say it differently?\"\n",
        "\n",
        "Explanation: The improved versions use natural and grammatically correct phrasing commonly used by native speakers. \"I apologize, I don't understand\" is a standard and polite way to indicate lack of comprehension. \"Sorry, I didn't get that. Can you say it differently?\" is a more informal but equally fluent alternative. Both options are significantly more fluent and easier to process than the original response."
      ],
      "metadata": {
        "id": "4vcJeA25KHcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Adversarial Testing Exercise:**\n"
      ],
      "metadata": {
        "id": "dlgv4CeAKHbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Adversarial Testing Exercise**\n",
        "\n",
        "Adversarial testing is used to challenge LLMs.\n",
        "Let us consider a simple factual question: \"What is the capitol of France?\".\n",
        "\n",
        "The expected answer is \"Paris\".\n",
        "\n",
        "A potential mistake an LLM might make is a factual error, like giving the wrong city.\n",
        "\n",
        "Such errors can arise from training data issues or hallucination.\n",
        "\n",
        "Several methods are suggested to improve robustness against these errors.\n",
        "\n",
        "- Retrieval-Augmented Generation (RAG) is one method, using external knowledge sources.\n",
        "\n",
        "- Fine-tuning on factual data can reinforce correct information. Implementing fact-checking mechanisms helps verify generated answers.\n",
        "\n",
        "- Confidence scoring allows flagging potentially incorrect answers.\n",
        "\n",
        "- Training with adversarial examples teaches the model to avoid common errors. It creates  tricky prompts to challenge LLMs, and test robustness, bias detection, or factual accuracy:\n",
        "-+ An example robustness challenge uses subtle negation regarding the Eiffel Tower.\n",
        "-- Bias detection can be challenged with open-ended prompts about stereotypes.\n",
        "-- Factual accuracy can be tested with prompts requiring niche or outdated information."
      ],
      "metadata": {
        "id": "1Vg_y55-Kxkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Comparative Analysis of Evaluation Methods:**\n",
        "\n",
        "Choose an NLP task (e.g., machine translation, text summarization, question answering). Compare and contrast at least three different evaluation metrics (BLEU, ROUGE, BERTScore, Perplexity, Human Evaluation, etc.). Discuss which metric is most appropriate for the chosen task and why."
      ],
      "metadata": {
        "id": "4DXjCrGW4Dax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Metrics for Text Summarization**\n",
        "\n",
        "| **Metric**      | **What It Measures**                                                         | **Strengths**                                                       | **Weaknesses**                                                            | **Best Use**                      |\n",
        "| --------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------- | --------------------------------- |\n",
        "| **ROUGE**       | Measures n-gram and<br>sequence overlap between<br>summary and reference     | ✔ Easy to compute<br>✔ Common in industry<br>✔ Reflects recall well | ✖ Misses paraphrasing<br>✖ Favors extractive style<br>✖ Ignores semantics | ✅ Widely accepted baseline        |\n",
        "| **BLEU**        | Measures n-gram precision<br>based on exact matches                          | ✔ Works for multiple refs<br>✔ Simple and fast                      | ✖ Not recall-based<br>✖ Poor for abstractive<br>✖ Ignores meaning         | 🚫 Better for translation         |\n",
        "| **BERTScore**   | Measures semantic similarity<br>using BERT embeddings                        | ✔ Captures paraphrases<br>✔ Correlates well with human evals        | ✖ Slower to compute<br>✖ Relies on BERT quality                           | ✅ Ideal for abstractive summaries |\n",
        "| **Perplexity**  | Measures how well a<br>language model predicts<br>the summary                | ✔ Good for fluency<br>✔ Model-internal metric                       | ✖ Doesn’t assess relevance<br>✖ Not suitable alone                        | ⚠️ Supplementary only             |\n",
        "| **Human Eval.** | Human judges rate summaries<br>on fluency, coherence,<br>and informativeness | ✔ Most accurate<br>✔ Understands nuance<br>✔ Evaluates meaning      | ✖ Time-consuming<br>✖ Costly<br>✖ Subjective                              | ✅ Best overall, use selectively   |\n"
      ],
      "metadata": {
        "id": "5RywV_i57Nyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion#\n",
        "\n",
        "Most Appropriate Metric for text summarization, especially abstractive summarization:\n",
        "\n",
        "ROUGE is a good starting point, still widely used due to its simplicity, speed, and reproducibility.\n",
        "\n",
        "BERTScore better captures the semantic quality of summaries produced by modern models (e.g., transformers) yet produces lexically different summaries.\n",
        "\n",
        "Human evaluation is the gold standard, especially for high-stakes application, when quality, nuance, and coherence are essential (e.g., for publication or production systems).\n",
        "\n",
        "Ideally, combination of ROUGE, BERTScore, and human assessment should be used for a comprehensive evaluation."
      ],
      "metadata": {
        "id": "nheXJDqE4iS_"
      }
    }
  ]
}