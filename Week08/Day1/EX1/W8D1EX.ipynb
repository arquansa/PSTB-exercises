{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi9+Y6XpaeKX9wj9jAtR42",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arquansa/PSTB-exercises/blob/main/Week08/Day1/EX1/W8D1EX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercises XP#\n",
        "\n",
        "\n",
        "ðŸŒŸ Exercise 1: Traditional vs. Modern NLP: A Comparative Analysis\n",
        "1. Create a table comparing and contrasting the traditional and modern NLP paradigms. Include the following aspects:\n",
        "\n",
        "Feature Engineering (manual vs. automatic)\n",
        "Word Representations (static vs. contextual)\n",
        "Model Architectures (shallow vs. deep)\n",
        "Training Methodology (task-specific vs. pre-training/fine-tuning)\n",
        "Key Examples of Models (e.g., NaÃ¯ve Bayes, BERT)\n",
        "Advantages and Disadvantages of each paradigm.\n",
        "2. Discuss how the evolution from traditional to modern NLP has impacted the scalability and efficiency of NLP applications.\n",
        "\n",
        "\n",
        "\n",
        "ðŸŒŸ Exercise 2: LLM Architecture and Application Scenarios\n",
        "For each of the following LLM architectures (BERT, GPT, T5), describe:\n",
        "\n",
        "The core architectural differences (e.g., bidirectional vs. unidirectional, masked language modeling vs. causal language modeling).\n",
        "A specific real-world application where that architecture excels.\n",
        "Explain why that specific architecture is well suited for that particular application.\n",
        "\n",
        "\n",
        "ðŸŒŸ Exercise 3: The Benefits and Ethical Considerations of Pre-training\n",
        "Explain in your own words the five key benefits of pre-trained models discussed in the lesson (improved generalization, reduced need for labeled data, faster fine-tuning, transfer learning, and robustness).\n",
        "Discuss potential ethical concerns associated with pre-training LLMs on massive datasets, such as bias, misinformation, and misuse.\n",
        "Propose potential mitigation strategies to address these ethical concerns.\n",
        "\n",
        "\n",
        "ðŸŒŸ Exercise 4 : Transformer Architecture Deep Dive\n",
        "Explain Self-Attention and Multi-Head Attention:\n",
        "\n",
        "Describe in detail how the self-attention mechanism works within a Transformer.\n",
        "Explain the purpose and advantages of multi-head attention compared to single-head attention.\n",
        "Provide a concrete example (different from the lesson) of a sentence and illustrate how multi-head attention might process it, focusing on different relationships between words.\n",
        "Pre-training Objectives:\n",
        "\n",
        "Compare and contrast Masked Language Modeling (MLM) and Causal Language Modeling (CLM).\n",
        "Describe a scenario where MLM would be more appropriate and a scenario where CLM would be more appropriate.\n",
        "Explain why early BERT models used Next Sentence Prediction (NSP) and why modern models tend to avoid it.\n",
        "Transformer Model Selection:\n",
        "\n",
        "You are tasked with building the following NLP applications. For each, specify which type of Transformer model (Encoder-only, Decoder-only, or Encoder-Decoder) would be most suitable and justify your choice.\n",
        "A system that analyzes customer reviews to determine if they are positive, negative, or neutral.\n",
        "A chatbot that can generate creative and engaging responses in a conversation.\n",
        "A service that automatically translates technical documents from English to Spanish.\n",
        "Explain the advantages of the chosen model type for each particular task.\n",
        "Positional Encoding:\n",
        "\n",
        "Explain the purpose of positional encoding, and why it is important for the transformer architecture.\n",
        "Give an example of a situation where the lack of positional encoding would cause a problem.\n",
        "\n",
        "\n",
        "ðŸŒŸ Exercise 5: BERT Variations - Choose Your Detective\n",
        "For each of the following scenarios, identify which BERT variation (RoBERTa, ALBERT, DistilBERT, ELECTRA, XLM-RoBERTa) would be most suitable and explain why:\n",
        "\n",
        "Scenario 1: Real-time sentiment analysis on mobile app with limited resources.\n",
        "Scenario 2: Research on legal documents requiring high accuracy.\n",
        "Scenario 3: Global customer support in multiple languages.\n",
        "Scenario 4: efficient pretraining, and token replacement detection.\n",
        "Scenario 5: efficient NLP in resource-constrained environments.\n",
        "Create a table comparing the key features and trade-offs of each BERT variation discussed in the lesson. Include aspects like:\n",
        "\n",
        "Training data and methods.\n",
        "\n",
        "Model size and efficiency.\n",
        "Specific optimizations and innovations.\n",
        "Ideal use cases.\n",
        "\n",
        "ðŸŒŸ Exercise 6: Softmax Temperature - The Randomness Regulator\n",
        "1. Temperature Scenarios: Describe how the output of a language model would differ in the following scenarios:\n",
        "\n",
        "Softmax temperature set to 0.2.\n",
        "Softmax temperature set to 1.5.\n",
        "Softmax temperature set to 1.\n",
        "2. Application Design:\n",
        "\n",
        "You are designing a system that generates personalized bedtime stories for children. Explain how you would use softmax temperature to control the creativity and coherence of the stories.\n",
        "You are building a system that automatically generates summaries of financial reports. Explain how you would use softmax temperature to ensure accuracy and reliability.\n",
        "Temperature and Bias:\n",
        "\n",
        "Discuss how adjusting softmax temperature might affect the potential for bias in a language modelâ€™s output.\n",
        "Give a practical example."
      ],
      "metadata": {
        "id": "JpMC_tLGmKzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1: Traditional vs. Modern NLP: A Comparative Analysis\n",
        "\n",
        "1. Create a table comparing and contrasting the traditional and modern NLP paradigms. Include the following aspects:\n",
        "\n",
        "Feature Engineering (manual vs. automatic)\n",
        "Word Representations (static vs. contextual)\n",
        "Model Architectures (shallow vs. deep)\n",
        "Training Methodology (task-specific vs. pre-training/fine-tuning)\n",
        "Key Examples of Models (e.g., NaÃ¯ve Bayes, BERT)\n",
        "Advantages and Disadvantages of each paradigm.\n"
      ],
      "metadata": {
        "id": "_3Fd_T3bmq4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Table: Traditional versus Modern NLP**"
      ],
      "metadata": {
        "id": "sh9a1fEmUTxT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UWhN0w3ofrZ"
      },
      "source": [
        "| Feature               | Traditional NLP                      | Modern NLP                                      |\n",
        "|-----------------------|--------------------------------------|-------------------------------------------------|\n",
        "| Feature Engineering   | Manual, rule-based, domain expertise | Automatic, learned from data                     |\n",
        "| Word Representations  | Static (e.g., Word2Vec, GloVe)        | Contextual (e.g., BERT, ELMo)                  |\n",
        "| Model Architectures   | Shallow models (e.g., SVM, NaÃ¯ve Bayes) | Deep neural networks (e.g., Transformers, RNNs) |\n",
        "| Training Methodology  | Task-specific training               | Pre-training on large datasets, fine-tuning on specific tasks |\n",
        "| Key Examples of Models | NaÃ¯ve Bayes, HMMs, CRFs              | BERT, GPT, T5                                   |\n",
        "| Advantages            | Interpretable, less data required (sometimes) | Higher accuracy, better generalization, handles complexity |\n",
        "| Disadvantages         | Requires significant manual effort, limited scalability | Less interpretable, requires large datasets and computational resources |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Discuss how the evolution from traditional to modern NLP has impacted the scalability and efficiency of NLP applications.\n",
        "\n",
        "The evolution from traditional to modern NLP greatly increased models efficiency: they can learn from massive datasets, adapt quicly to new tasks. Deep learning, pre-training, automated feature engineering, and fine-tuning all contributed to boost modern NLP."
      ],
      "metadata": {
        "id": "o0pfeoS4pnAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2 LLM Architecture and Application Scenarios#\n",
        "\n",
        "LLM Architecture and Application Scenarios For each of the following LLM architectures (BERT, GPT, T5), describe:\n",
        "\n",
        "The core architectural differences (e.g., bidirectional vs. unidirectional, masked language modeling vs. causal language modeling). A specific real-world application where that architecture excels. Explain why that specific architecture is well suited for that particular application."
      ],
      "metadata": {
        "id": "_EBfmKKDt9vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table: A comparison of BERT, GPT, and T5 LLM architectures - diffrences and specific applications"
      ],
      "metadata": {
        "id": "g86l8rJQUo5A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-YB6dEju8f9"
      },
      "source": [
        "| LLM Architecture | Core Architectural Differences                                  | Specific Real-World Application        | Reason for Suitability                                                                 |\n",
        "|-----------------|---------------------------------------------------------------|----------------------------------------|----------------------------------------------------------------------------------------|\n",
        "| BERT            | Bidirectional, Masked Language Modeling (MLM)               | Sentiment Analysis                     | Processes text from both directions, capturing context effectively for classification. |\n",
        "| GPT             | Unidirectional, Causal Language Modeling (CLM)                | Text Generation (e.g., creative writing) | Generates text sequentially, making it suitable for creative and coherent content.     |\n",
        "| T5              | Encoder-Decoder, Text-to-Text framework (various objectives) | Machine Translation                    | Its encoder-decoder structure is ideal for sequence-to-sequence tasks like translation. |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3 The Benefits and Ethical Considerations of Pre-training#\n",
        "\n",
        "Explain in your own words the five key benefits of pre-trained models discussed in the lesson (improved generalization, reduced need for labeled data, faster fine-tuning, transfer learning, and robustness). Discuss potential ethical concerns associated with pre-training LLMs on massive datasets, such as bias, misinformation, and misuse. Propose potential mitigation strategies to address these ethical concerns.\n",
        "\n",
        "When it comes to models pre-training in NLP, specific advantages and ethical considerations are at stake:\n",
        "- Data pre-training enables processing of great quantities of data, improves generalization, reduces need for labelled data, accelerates fine tuning, and transfer learning.\n",
        "- Yet, pre-training also raises ethical concerns for instance, the risk of spreading fake news, biased viewpoints, and more generally of generating misinformation, possibly for harmful purposes.\n",
        "To address these problems, careful data curation and bias mitigation techniques must be implemented, ethical guidelines must be established, and users must be educated on responsible AI use."
      ],
      "metadata": {
        "id": "E3mCp_4-pmYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4 Transformer Architecture Deep Dive#\n",
        "\n",
        " Explain Self-Attention and Multi-Head Attention#\n",
        " Describe in detail how the self-attention mechanism works within a Transformer. Explain the purpose and advantages of multi-head attention compared to single-head attention. Provide a concrete example (different from the lesson) of a sentence and illustrate how multi-head attention might process it, focusing on different relationships between words. Pre-training Objectives:\n",
        "\n",
        "Compare and contrast Masked Language Modeling (MLM) and Causal Language Modeling (CLM). Describe a scenario where MLM would be more appropriate and a scenario where CLM would be more appropriate. Explain why early BERT models used Next Sentence Prediction (NSP) and why modern models tend to avoid it. Transformer Model Selection:\n",
        "\n",
        "\n",
        "Table: Comparison of MLM and CLM modelling;  NSP - BERT models versus modern models\n",
        "\n",
        "\n",
        "| **Application**                             | **Best Model Type** | **Justification**                                                                                                        | **Advantages**                                                               |\n",
        "| ------------------------------------------- | ------------------- | ------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------- |\n",
        "| **Sentiment Analysis on Reviews**           | Encoder-only        | Requires understanding full text to classify sentiment. Encoder models like BERT excel at extracting contextual meaning. | Bidirectional context, accurate classification, efficient fine-tuning.       |\n",
        "| **Creative Chatbot for Conversations**      | Decoder-only        | Needs fluent, sequential text generation. Decoder models like GPT generate responses auto-regressively.                  | Natural language generation, handles open-ended dialogue, creativity.        |\n",
        "| **English-to-Spanish Document Translation** | Encoder-Decoder     | Requires transforming source text to target language. Encoder-Decoder models process and generate sequences effectively. | High translation quality, handles complex text, supports multiple languages. |\n",
        "\n",
        " Positional Encoding (Briefly)\n",
        " Why Positional Encoding Is Important in Transformers:\n",
        "Transformers, unlike RNNs or CNNs,  lack recurrence. They do not have a built-in sense of sequence order, process input tokens in parallel, which improves efficiency but requires explicit information about token positions to understand word order and sequence structure.\n",
        "\n",
        "ðŸ”¹ Positional encoding solves this by injecting information about each tokenâ€™s position in the input sequence into the model (adding a position vector to each token embedding).\n",
        "Transformers use positional encoding to inject word order information, since they lack recurrence. Thus the model can differentiate word positions (e.g., â€œThe dog chased the catâ€ â‰  â€œThe cat chased the dogâ€), if not, the system would not know in both situations who is the chaser."
      ],
      "metadata": {
        "id": "90J4HTMY4TyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Architecture Deep Dive Explain Self-Attention and Multi-Head Attention:\n",
        "\n",
        "Describe in 2 or 3 lines how the self-attention mechanism works within a Transformer. Explain the purpose and advantages of multi-head attention compared to single-head attention\n",
        "\n",
        "Self-Attention in a Transformer allows each token in a sequence to weigh and attend to every other token, capturing contextual relationships by computing attention scores using query, key, and value vectors. This mechanism helps the model understand dependencies regardless of distance in the input.\n",
        "\n",
        "Running multiple self-attention operations in parallel with different learned projections, Multi-Head Attention can capture diverse types of relationships and patterns simultaneouslyâ€”something a single head might miss.\n"
      ],
      "metadata": {
        "id": "TNfkKxMd5_9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5  BERT Variations#\n",
        "\n",
        "Table: A comparison of BERT variations - training method, model size efficiency, optimization & ideal use\n",
        "\n",
        "\n",
        "| **Model**       | **Training Data & Method**                 | **Model Size & Efficiency**    | **Optimizations & Innovations**                 | **Ideal Use Cases**                                                       |\n",
        "| --------------- | ------------------------------------------ | ------------------------------ | ----------------------------------------------- | ------------------------------------------------------------------------- |\n",
        "| **RoBERTa**     | More data, dynamic masking, no NSP         | Large; less efficient          | Robust pretraining; optimized training schedule | High-accuracy NLP tasks, research, legal text analysis                    |\n",
        "| **ALBERT**      | Factorized embeddings, parameter sharing   | Very compact; highly efficient | Reduced size with similar performance           | Large-scale NLP with low memory; research                                 |\n",
        "| **DistilBERT**  | Distilled from BERT; reduced layers        | 40% smaller, 60% faster        | Knowledge distillation for speed                | Real-time inference (e.g., mobile apps, sentiment analysis)               |\n",
        "| **ELECTRA**     | Replaces MLM with replaced token detection | Efficient; better than BERT    | Discriminator-based pretraining                 | Efficient pretraining; real-time tasks; resource-constrained environments |\n",
        "| **XLM-RoBERTa** | Trained on 100+ languages                  | Large; slower inference        | Multilingual, robust tokenization               | Global customer support, multilingual applications                        |\n",
        "\n"
      ],
      "metadata": {
        "id": "2G76pElUcD3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 6: Softmax Temperature - The Randomness Regulator#\n",
        "\n",
        "The softmax temperature controls the randomness or creativity of a language model's output by scaling the logits before applying the softmax function. Here's how output behavior changes at different temperature values:\n",
        "\n",
        "\n",
        "Table: Impact of softmax temperature on randomness or creativity of a language model's output\n",
        "\n",
        "| **Temperature** | **Effect on Output**         | **Behavior**                                                                 |\n",
        "| --------------- | ---------------------------- | ---------------------------------------------------------------------------- |\n",
        "| **0.2**         | **Very deterministic**       | Strong preference for high-probability tokens â†’ repetitive, predictable text |\n",
        "| **1**           | **Balanced (default)**       | Natural, coherent, with moderate variability â†’ standard human-like output    |\n",
        "| **1.5**         | **Highly random / creative** | More diverse, but potentially incoherent or off-topic                      |\n",
        "\n",
        "\n",
        "Application Design:\n",
        "Table: Impact of temperature on output and bias setting\n",
        "\n",
        "| **System**                               | **Use of Temperature**                       | **Effect on Output**                                                    | **Bias Setting**                                                              |\n",
        "| ---------------------------------------- | -------------------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
        "| **Personalized Bedtime Story Generator** | Use a **higher temperature** (e.g., 0.8â€“1.2) | Increases randomness and creativity in word choice and story direction. | May lower bias slightly to allow more variety in themes and characters.       |\n",
        "| **Financial Report Summarizer**          | Use a **lower temperature** (e.g., 0.2â€“0.5)  | Produces more focused, factual, and deterministic summaries.            | Set bias to **favor formal, factual tokens** to enhance accuracy and clarity. |\n",
        "\n",
        "---\n",
        "* **Temperature** modifies the softmax distribution:\n",
        "\n",
        "  * **High temp (>1)**: flattens the distribution and gives more diverse word choices.\n",
        "  * **Low temp (<1)**: sharpens the distribution and will induce a more predictable, focused output.\n",
        "* **Bias** adjusts token likelihoods directly, helping guide tone, domain, or precision.\n",
        "\n",
        "A practical example\n",
        "\n",
        "| **Temperature** | **Generated Variant**                                                                                         |\n",
        "| --------------- | ------------------------------------------------------------------------------------------------------------- |\n",
        "| **0.2**         | *The prince rode a white horse. He saw the princess and greeted her politely.*                                |\n",
        "| **0.7**         | *The prince was riding a white horse when he spotted a princess waving by the river.*                         |\n",
        "| **1.0**         | *The prince galloped through moonlit meadows on his ivory steed and glimpsed a princess dancing in stardust.* |\n",
        "| **1.3**         | *The prince soared on a candy-colored unicorn and found a princess painting clouds.*                          |\n"
      ],
      "metadata": {
        "id": "JGzZRdHRjZwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As a conclusion:**\n",
        "The more temperature increases, the more creative and poetic the output, while coherence and realism may decrease, especially if temperature >1.0"
      ],
      "metadata": {
        "id": "NUYK3JnSp0GA"
      }
    }
  ]
}