{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arquansa/PSTB-exercises/blob/main/Week07/Day3/DC3/W7D3DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d339d57",
      "metadata": {
        "id": "3d339d57"
      },
      "source": [
        "#Daily Challenge: Build a Retrieval Augmented Generation (RAG) System#\n",
        "Mandatory : You must read this article before starting the exercise Faiss | LangChain\n",
        "\n",
        "Mandatory : You must watch these videos before starting the exercise\n",
        "\n",
        "PyTorch in 100 Seconds\n",
        "\n",
        "#Task\n",
        "Our task is to implement RAG using Langchain and Hugging Face!\n",
        "\n",
        "Set up your environment: This ensures all the necessary tools are available to build the RAG system. Each library serves a specific role: Langchain handles the orchestration of components, transformers provide pre-trained models, sentence-transformers generate embeddings, datasets load sample data, and FAISS enables fast similarity searches.\n",
        "Open your terminal or notebook environment. Install all required libraries by running these commands:\n",
        "\n",
        "Load the dataset: To provide the system with information to retrieve from, you‚Äôll load a real-world dataset. HuggingFaceDatasetLoader simplifies the process of accessing Hugging Face datasets and formatting them into documents that Langchain can process.\n",
        "before loading the dataset, run :\n",
        "\n",
        "My Courses Calendar Leaderboard About DI Terms & Conditions Privacy Menu My Courses My Achievements My Trophy My Diploma Refer Friend My Payment Resume Matcher P2P Rooms My OctoHelp ‚ú® AI Assistant GenAI & Machine Learning Bootcamp 2025 - Full Time 2025 - PSTB NLP and LLM BERT and RAG: Contextual Embeddings for Enhanced LLMs Build a (RAG) System Build a (RAG) System Last Updated: July 10th, 2025\n",
        "\n",
        "Daily Challenge: Build a Retrieval Augmented Generation (RAG) System\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn Implement a Retrieval Augmented Generation (RAG) system using Langchain and Hugging Face. Load and process datasets using Hugging Face datasets and Langchain HuggingFaceDatasetLoader. Split documents into smaller chunks using Langchain RecursiveCharacterTextSplitter. Generate text embeddings using Hugging Face sentence-transformers and Langchain HuggingFaceEmbeddings. Create and utilize vector stores with Langchain FAISS for efficient document retrieval. Prepare and integrate a pre-trained Language Model (LLM) from Hugging Face transformers for question answering. Build a Retrieval QA Chain using Langchain RetrievalQA to answer questions based on retrieved documents.\n",
        "\n",
        "üõ†Ô∏è What you will create You will create a functional RAG system that can answer questions based on a dataset loaded from Hugging Face Datasets. This system will:\n",
        "\n",
        "Load the databricks/databricks-dolly-15k dataset. Index the dataset content into a vector store. Utilize a pre-trained question-answering model from Hugging Face. Answer user queries by retrieving relevant documents and using the LLM to generate answers.\n",
        "\n",
        "Mandatory : You must read this article before starting the exercise Faiss | LangChain\n",
        "\n",
        "Mandatory : You must watch these videos before starting the exercise\n",
        "\n",
        "PyTorch in 100 Seconds\n",
        "\n",
        "LangChain Explained in 13 Minutes\n",
        "\n",
        "Task Our task is to implement RAG using Langchain and Hugging Face!\n",
        "\n",
        "Set up your environment: : This ensures all the necessary tools are available to build the RAG system. Each library serves a specific role: Langchain handles the orchestration of components, transformers provide pre-trained models, sentence-transformers generate embeddings, datasets load sample data, and FAISS enables fast similarity searches.\n",
        "Open your terminal or notebook environment. Install all required libraries by running these commands:\n",
        "\n",
        "!pip install -q langchain !pip install -q torch !pip install -q transformers !pip install -q sentence-transformers !pip install -q datasets !pip install -q faiss-cpu !pip install -U langchain-community\n",
        "\n",
        "Load the dataset: To provide the system with information to retrieve from, you‚Äôll load a real-world dataset. HuggingFaceDatasetLoader simplifies the process of accessing Hugging Face datasets and formatting them into documents that Langchain can process.\n",
        "before loading the dataset, run : pip install -Uq datasets Import HuggingFaceDatasetLoader from langchain.document_loaders. Specify the dataset name and content column:\n",
        "\n",
        "dataset_name = \"databricks/databricks-dolly-15k\" page_content_column = \"context\"\n",
        "\n",
        "Create a HuggingFaceDatasetLoader instance and load the data as documents:\n",
        "\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column) data = loader.load() print(data[:2]) # Optional: Print the first 2 entries to verify loading\n",
        "\n",
        "Split the documents: Language models have a limit on how much text they can process at once. Splitting large documents into smaller, overlapping chunks ensures that no important context is lost and that each piece of text is a manageable size for embedding and retrieval.\n",
        "Import RecursiveCharacterTextSplitter from langchain.text_splitter. Create a RecursiveCharacterTextSplitter instance with a chunk_size of 1000 and chunk_overlap of 150:\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "Split the loaded documents:\n",
        "\n",
        "docs = text_splitter.split_documents(data) print(docs[0]) # Optional: Print the first document chunk\n",
        "\n",
        "Embed the text: Text needs to be converted into numerical representations (embeddings) so that similar pieces of text can be found efficiently. Using a sentence-transformer model creates embeddings that capture semantic meaning, enabling effective retrieval later.\n",
        "Import HuggingFaceEmbeddings from langchain.embeddings. Define the model path, model configurations, and encoding options:\n",
        "\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\" model_kwargs = {'device':'cpu'} encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "Initialize HuggingFaceEmbeddings:\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings( model_name=modelPath, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs )\n",
        "\n",
        "(Optional) Test embedding creation:\n",
        "\n",
        "text = \"This is a test document.\" query_result = embeddings.embed_query(text) print(query_result[:3])\n",
        "\n",
        "Create a vector store: A vector store like FAISS indexes the embeddings, allowing fast and scalable similarity searches. This is how the system quickly finds relevant pieces of text when a query is made.\n",
        "Import FAISS from langchain.vectorstores. Create a FAISS vector store from the document chunks and embeddings:\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "Note: This step might take some time depending on your dataset size.\n",
        "\n",
        "Prepare the LLM model: The Language Model is responsible for generating answers based on retrieved documents. Loading a pre-trained model and wrapping it in a Langchain pipeline makes it easy to integrate with the retrieval system.\n",
        "Import necessary classes from transformers and langchain:\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline from langchain import HuggingFacePipeline\n",
        "\n",
        "Load the tokenizer and question-answering model:\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\") model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n",
        "\n",
        "Create a question-answering pipeline:\n",
        "\n",
        "model_name = \"Intel/dynamic_tinybert\" tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512) Youtubeer = pipeline( \"question-answering\", model=model_name, tokenizer=tokenizer, return_tensors='pt' )\n",
        "\n",
        "Create a Langchain pipeline wrapper:\n",
        "\n",
        "llm = HuggingFacePipeline( pipeline=Youtubeer, model_kwargs={\"temperature\": 0.7, \"max_length\": 512}, )\n",
        "\n",
        "Build the Retrieval QA Chain: The Retrieval QA Chain connects the retriever (which finds relevant documents) with the LLM (which generates answers). This chain enables the full RAG process, where the system retrieves helpful context and then answers the user‚Äôs query based on that context.\n",
        "Import RetrievalQA from langchain.chains. Create a retriever from your FAISS database:\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4}) # Optional: You can adjust k for number of documents retrieved\n",
        "\n",
        "Build the RetrievalQA chain:\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)\n",
        "\n",
        "Test your RAG system: Running a test query allows you to verify that all components are working together. This step ensures that documents are retrieved correctly and that the model generates meaningful answers based on the retrieved context.\n",
        "Define your question:\n",
        "\n",
        "question = \"What is cheesemaking?\"\n",
        "\n",
        "Run the QA chain and print the result:\n",
        "\n",
        "result = qa.run({\"query\": question}) print(result) # Or print(result[\"result\"]) if the output is a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up the environment#"
      ],
      "metadata": {
        "id": "FNVZ1XlFnZf7"
      },
      "id": "FNVZ1XlFnZf7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be9f797b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be9f797b",
        "outputId": "851c3044-3dda-4103-d4ed-7a61acd208a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.71)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the dataset#"
      ],
      "metadata": {
        "id": "yE2I362QnwXR"
      },
      "id": "yE2I362QnwXR"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq datasets"
      ],
      "metadata": {
        "id": "Dg0OGKlyNqUC"
      },
      "id": "Dg0OGKlyNqUC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import HuggingFaceDatasetLoader"
      ],
      "metadata": {
        "id": "vHzc-RXPNqEM"
      },
      "id": "vHzc-RXPNqEM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1cae65e",
      "metadata": {
        "id": "d1cae65e"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "page_content_column = \"context\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6445276a",
      "metadata": {
        "id": "6445276a"
      },
      "source": [
        "**Create a HuggingFaceDatasetLoader instance and load the data as documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708b1280",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "708b1280",
        "outputId": "1736463d-4bb3-4be6-b4b6-228d227703eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}, page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"'), Document(metadata={'instruction': 'Which is a species of fish? Tope or Rope', 'response': 'Tope', 'category': 'classification'}, page_content='\"\"')]\n"
          ]
        }
      ],
      "source": [
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
        "data = loader.load()\n",
        "print(data[:2]) # Optional: Print the first 2 entries to verify loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4357e055",
      "metadata": {
        "id": "4357e055"
      },
      "source": [
        "#Split the documents#\n",
        "Import RecursiveCharacterTextSplitter from langchain.text_splitter.\n",
        "Create a RecursiveCharacterTextSplitter instance with a chunk_size of 1000 and chunk_overlap of 150:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "52Bhx64lR9Z3"
      },
      "id": "52Bhx64lR9Z3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)"
      ],
      "metadata": {
        "id": "eUUUK_WOS5Kp"
      },
      "id": "eUUUK_WOS5Kp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a3d57637",
      "metadata": {
        "id": "a3d57637"
      },
      "source": [
        "**Split the loaded documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e8036d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33e8036d",
        "outputId": "e4fcd15f-28b2-4cbe-c4d9-4f7ef82ed1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"' metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n"
          ]
        }
      ],
      "source": [
        "docs = text_splitter.split_documents(data)\n",
        "print(docs[0]) # Optional: Print the first document chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97509448",
      "metadata": {
        "id": "97509448"
      },
      "source": [
        "#Embed the text#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "MTai3ndrTbnk"
      },
      "id": "MTai3ndrTbnk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}"
      ],
      "metadata": {
        "id": "4dmsc4DzTalK"
      },
      "id": "4dmsc4DzTalK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0fc6e976",
      "metadata": {
        "id": "0fc6e976"
      },
      "source": [
        "**Initialize HuggingFaceEmbeddings**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name=modelPath,\n",
        "  model_kwargs=model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "XCLkRlK6UGMi"
      },
      "id": "XCLkRlK6UGMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3d1a2de8",
      "metadata": {
        "id": "3d1a2de8"
      },
      "source": [
        "**(Optional) Test embedding creation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a test document.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "print(query_result[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KudPBg0UdUf",
        "outputId": "793a73a3-211e-41c0-9361-0828f0c55041"
      },
      "id": "-KudPBg0UdUf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.038338541984558105, 0.12346471846103668, -0.02864297851920128]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db05edc1",
      "metadata": {
        "id": "db05edc1"
      },
      "source": [
        "#Create a vector store#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bcb4fb2",
      "metadata": {
        "id": "7bcb4fb2"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "QMRXvhM0Va7f"
      },
      "id": "QMRXvhM0Va7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the LLM mode"
      ],
      "metadata": {
        "id": "RZifO8dOZYlX"
      },
      "id": "RZifO8dOZYlX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary classes from transformers and langchain**"
      ],
      "metadata": {
        "id": "xLa6uTlLaeLw"
      },
      "id": "xLa6uTlLaeLw"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering"
      ],
      "metadata": {
        "id": "XpPwWceIaQli"
      },
      "id": "XpPwWceIaQli",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the tokenizer and question-answering model**"
      ],
      "metadata": {
        "id": "oYp7ncLaagSF"
      },
      "id": "oYp7ncLaagSF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to a conflict between Huggingface and LangChain, parts of the existing code had to be modified, hence the green code comments(#)."
      ],
      "metadata": {
        "id": "3BFNCnyEbc6o"
      },
      "id": "3BFNCnyEbc6o"
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")"
      ],
      "metadata": {
        "id": "b5nSMXgpeUWP"
      },
      "id": "b5nSMXgpeUWP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b855dbbd",
      "metadata": {
        "id": "b855dbbd"
      },
      "source": [
        "**Create a question-answering pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SGu-aJDYiSA",
        "outputId": "0fba9591-269e-4f56-e1d9-a89ac06c1fc8"
      },
      "id": "0SGu-aJDYiSA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Langchain pipeline wrapper**"
      ],
      "metadata": {
        "id": "MW0Hf4wElr9M"
      },
      "id": "MW0Hf4wElr9M"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "xj85BE0wYum4"
      },
      "id": "xj85BE0wYum4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_name = \"Intel/dynamic_tinybert\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
        "#Youtubeer = pipeline(\n",
        "#  \"question-answering\",\n",
        "#  model=model_name,\n",
        "#  tokenizer=tokenizer,\n",
        "# return_tensors='pt'\n",
        "#)"
      ],
      "metadata": {
        "id": "TvLOKa6K8hiw"
      },
      "id": "TvLOKa6K8hiw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm = HuggingFacePipeline(\n",
        "  #pipeline=Youtubeer,\n",
        "  #model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
        "#)"
      ],
      "metadata": {
        "id": "Ca4N3-3LfOuM"
      },
      "id": "Ca4N3-3LfOuM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "5WnTqf4seJJr"
      },
      "id": "5WnTqf4seJJr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the RetrievalQA chain:#"
      ],
      "metadata": {
        "id": "VeW4zc9vm9Zm"
      },
      "id": "VeW4zc9vm9Zm"
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 4}) # Optional: You can adjust k for number of documents retrieved"
      ],
      "metadata": {
        "id": "08KoTTCRnP_3"
      },
      "id": "08KoTTCRnP_3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",  # ou \"stuff\"\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "rHHHpEGVeIbR"
      },
      "id": "rHHHpEGVeIbR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "68bd7d94",
      "metadata": {
        "id": "68bd7d94"
      },
      "source": [
        "#Test your RAG system#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da54dc7a",
      "metadata": {
        "id": "da54dc7a"
      },
      "source": [
        "**Define the question**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is cheesemaking?\""
      ],
      "metadata": {
        "id": "obKUI3pqjBnH"
      },
      "id": "obKUI3pqjBnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  result = qa({\"query\": question})\n",
        "print(\"R√©ponse:\", result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd7Ew5L3jBYR",
        "outputId": "e06a15d1-32e2-4299-dfce-d60420aaaf2d"
      },
      "id": "Yd7Ew5L3jBYR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R√©ponse: nnCulturingnCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the release of cheese whey in a process known as 'cheddaring'. nnCulturingnCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the release of cheese whey in a process known as 'cheddaring'. nnCulturingnCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the release of cheese whey in a process known as 'cheddaring'. nnCulturingnCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the release of cheese whey in a process known as 'cheddaring'. nnCulturingnCheese is made by bringing milk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "After testing the RAG, it is to be noted that we get an answer but due to the sequence length that was longer than max length expected (600 instead of 512), there are defects on word separators."
      ],
      "metadata": {
        "id": "XcLeGwepesZ6"
      },
      "id": "XcLeGwepesZ6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}