{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgqBXDKTM0E3OcTCzIChbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arquansa/PSTB-exercises/blob/main/Week06/Day1/EX1/W6D1EXG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises XP Gold Last Updated: February 14th, 2025\n",
        "\n",
        "ðŸ‘©â€ðŸ« ðŸ‘©ðŸ¿â€ðŸ« What Youâ€™ll learn\n",
        "\n",
        "- How neural networks make predictions using forward propagation\n",
        "- How backpropagation adjusts weights to minimize errors\n",
        "- How activation functions impact neural network performance\n",
        "- How to manually compute and implement gradient descent\n",
        "- How to build and train a simple neural network from scratch\n",
        "ðŸ› ï¸ What you will create\n",
        "\n",
        "- A multi-layer perceptron (MLP) for binary classification\n",
        "- A Python implementation of activation functions\n",
        "- A neural network that predicts house prices using forward propagation\n",
        "- A model that updates weights using gradient descent\n",
        "- A simple training loop for a neural network\n",
        "\n",
        "Exercise 1: Implementing a Multi-Layer Perceptron (MLP) from Scratch\n",
        "\n",
        "Task\n",
        "\n",
        "You will build a multi-layer perceptron for a binary classification problem using NumPy.\n",
        "\n",
        "The model should:\n",
        "\n",
        "- Have one hidden layer with three neurons\n",
        "- Use ReLU activation for hidden layers and Sigmoid for the output layer\n",
        "- Compute forward propagation\n",
        "\n",
        "Steps\n",
        "\n",
        "Define the architecture:\n",
        "- Input layer with two neurons\n",
        "- Hidden layer with three neurons using ReLU activation\n",
        "- Output layer with one neuron using Sigmoid activation\n",
        "Implement forward propagation using weighted sum\n",
        "- calculations and activation functions\n",
        "- Predict outputs for the following inputs:\n",
        "- First case: Input values of two and three\n",
        "- Second case: Input values of one and five\n",
        "\n",
        "Exercise 2: Implementing Backpropagation with Gradient Descent\n",
        "\n",
        "Task\n",
        "\n",
        "You will manually compute backpropagation and update weights\n",
        "\n",
        "- using gradient descent for a simple neural network\n",
        "- predicting student exam scores.\n",
        "\n",
        "Given Data\n",
        "\n",
        "Input variables:\n",
        "\n",
        "- Study hours with a value of six\n",
        "- Previous test score with a value of seventy-five\n",
        "\n",
        "Initial weights:\n",
        "\n",
        "- First weight with a value of zero point four for study hours\n",
        "- Second weight with a value of zero point six for previous test score\n",
        "\n",
        "- Bias with a value of five\n",
        "\n",
        "- Actual exam score with a value of eighty-five\n",
        "Learning rate of zero point zero one\n",
        "\n",
        "Steps\n",
        "- Compute the predicted exam score using the weighted sum formula\n",
        "- Calculate the loss using the mean squared error formula\n",
        "- Compute the gradients for the weights and update them using gradient descent\n",
        "- Interpret the results and observe how much the weights change\n",
        "\n",
        "Exercise 3: Comparing Activation Functions for Neural Networks\n",
        "\n",
        "Task\n",
        "\n",
        "You will implement and compare three activation functions using Python:\n",
        "\n",
        "Step\n",
        "- function Sigmoid function\n",
        "- ReLU function\n",
        "Steps\n",
        "- Implement the three activation functions in Python\n",
        "- Create a graph that visualizes the Sigmoid and ReLU functions\n",
        "- Compare the behaviors of the activation functions:\n",
        "- Identify which function gives only binary outputs\n",
        "- Identify which function smoothly transitions between values\n",
        "- Identify which function sets negative values to zero but keeps positive values unchanged\n",
        "- Answer the following questions:\n",
        "- Why is ReLU commonly used in deep learning models?\n",
        "- Why might Sigmoid be a good choice for binary classification tasks?\n",
        "- What are the weaknesses of the Step function compared to others?\n",
        "\n",
        "Exercise 4: Forward Propagation in a Deep Neural Network\n",
        "\n",
        "Task\n",
        "You will manually compute forward propagation for a three-layer neural network predicting house prices.\n",
        "\n",
        "Given Data\n",
        "\n",
        "Inputs:\n",
        "\n",
        "Square footage with a value of two thousand\n",
        "Number of bedrooms with a value of three Layer one weights and bias:\n",
        "- First weight with a value of zero point five\n",
        "- Second weight with a value of zero point seven\n",
        "- First bias with a value of ten thousand\n",
        "Layer two weights and bias:\n",
        "- First weight with a value of zero point six\n",
        "- Second weight with a value of zero point eight\n",
        "- Second bias with a value of twenty thousand\n",
        "Output layer weight and bias:\n",
        "- Weight with a value of one point two\n",
        "- Bias with a value of thirty thousand Activation function: - ReLU\n",
        "\n",
        "Steps\n",
        "- Compute the output of the first layer using the weighted sum formula and ReLU activation function\n",
        "- Compute the output of the second layer using the same method\n",
        "- Compute the final prediction using the weighted sum formula at the output layer\n",
        "- Interpret the final result and determine the predicted house price\n",
        "\n",
        "Exercise 5: Training a Neural Network with Forward and Backward Propagation\n",
        "\n",
        "Task You will train a simple neural network to predict a studentâ€™s exam score using gradient descent.\n",
        "\n",
        "Steps\n",
        "\n",
        "- Initialize input values representing study hours and previous test score\n",
        "- Initialize weights and bias with given values\n",
        "- Compute forward propagation to predict the exam score\n",
        "- Compute the error between the prediction and actual score\n",
        "- Compute gradients for the weights and bias\n",
        "- Update the weights and bias using gradient descent\n",
        "- Print the updated weights and bias after one training iteration\n",
        "\n",
        "Conclusion\n",
        "\n",
        "By completing these exercises, you will:\n",
        "- Understand multi-layer perceptrons\n",
        "- Implement activation functions\n",
        "- Learn forward and backward propagation\n",
        "- Apply gradient descent to train a neural network"
      ],
      "metadata": {
        "id": "JgQHxkeoUTFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implementing a Multi-Layer Perceptron (MLP) from Scratch"
      ],
      "metadata": {
        "id": "sn0Nv15AYxHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Implementing a Multi-Layer Perceptron (MLP) from Scratch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the architecture\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42) # for reproducibility\n",
        "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "bias_hidden = np.random.randn(1, hidden_size)\n",
        "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "bias_output = np.random.randn(1, output_size)\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(inputs, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output):\n",
        "    # Input to hidden layer\n",
        "    hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "    # Hidden to output layer\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = sigmoid(output_layer_input)\n",
        "\n",
        "    return predicted_output\n",
        "\n",
        "# Predict outputs for the given inputs\n",
        "inputs_case1 = np.array([[2, 3]])\n",
        "prediction_case1 = forward_propagation(inputs_case1, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n",
        "print(f\"Prediction for input [2, 3]: {prediction_case1}\")\n",
        "\n",
        "inputs_case2 = np.array([[1, 5]])\n",
        "prediction_case2 = forward_propagation(inputs_case2, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n",
        "print(f\"Prediction for input [1, 5]: {prediction_case2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_86vffsxZIS",
        "outputId": "f2de665c-efe4-4dbb-8c94-4fa372fcbba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for input [2, 3]: [[0.98303123]]\n",
            "Prediction for input [1, 5]: [[0.99592967]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00daaa6d"
      },
      "source": [
        "## Exercise 2: Implementing Backpropagation with Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9debd2d7",
        "outputId": "070c53ed-c766-4fa7-abbe-7ebec3fb1a55"
      },
      "source": [
        "# Exercise 2: Implementing Backpropagation with Gradient Descent\n",
        "\n",
        "# Given Data\n",
        "study_hours = 6\n",
        "previous_score = 75\n",
        "weight_study = 0.4\n",
        "weight_previous = 0.6\n",
        "bias = 5\n",
        "actual_score = 85\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 1. Compute the predicted exam score using the weighted sum formula\n",
        "predicted_score = (study_hours * weight_study) + (previous_score * weight_previous) + bias\n",
        "print(f\"Predicted exam score: {predicted_score}\")\n",
        "\n",
        "# 2. Calculate the loss using the mean squared error formula\n",
        "loss = (actual_score - predicted_score)**2\n",
        "print(f\"Mean Squared Error Loss: {loss}\")\n",
        "\n",
        "# 3. Compute the gradients for the weights and update them using gradient descent\n",
        "# The derivative of the loss function (MSE) with respect to the output is 2 * (predicted_score - actual_score)\n",
        "# The derivative of the output with respect to weight_study is study_hours\n",
        "# The derivative of the output with respect to weight_previous is previous_score\n",
        "# The derivative of the output with respect to bias is 1\n",
        "gradient_weight_study = (predicted_score - actual_score) * study_hours * 2 # Chain rule\n",
        "gradient_weight_previous = (predicted_score - actual_score) * previous_score * 2 # Chain rule\n",
        "gradient_bias = (predicted_score - actual_score) * 2 # Chain rule\n",
        "\n",
        "print(f\"Gradient for weight_study: {gradient_weight_study:.4f}\")\n",
        "print(f\"Gradient for weight_previous: {gradient_weight_previous:.4f}\")\n",
        "print(f\"Gradient for bias: {gradient_bias:.4f}\")\n",
        "\n",
        "# Update weights and bias\n",
        "weight_study = weight_study - learning_rate * gradient_weight_study\n",
        "weight_previous = weight_previous - learning_rate * gradient_weight_previous\n",
        "bias = bias - learning_rate * gradient_bias\n",
        "\n",
        "# 4. Interpret the results and observe how much the weights change\n",
        "print(f\"Updated weight_study: {weight_study:.4f}\")\n",
        "print(f\"Updated weight_previous: {weight_previous:.4f}\")\n",
        "print(f\"Updated bias: {bias:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted exam score: 52.4\n",
            "Mean Squared Error Loss: 1062.76\n",
            "Gradient for weight_study: -391.2000\n",
            "Gradient for weight_previous: -4890.0000\n",
            "Gradient for bias: -65.2000\n",
            "Updated weight_study: 4.3120\n",
            "Updated weight_previous: 49.5000\n",
            "Updated bias: 5.6520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28371d1"
      },
      "source": [
        "## Exercise 3: Comparing Activation Functions for Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Activation functions\n",
        "def step_function(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "Qxvw06ry-nkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "a9cdce02",
        "outputId": "b0784027-b2a8-46da-e1ab-97f4762335d4"
      },
      "source": [
        "# Create a range of input values for visualization\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Calculate the output of each activation function\n",
        "y_step = step_function(x)\n",
        "y_sigmoid = sigmoid(x)\n",
        "y_relu = relu(x)\n",
        "\n",
        "# Create a graph that visualizes the Sigmoid and ReLU functions\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(x, y_sigmoid, label='Sigmoid')\n",
        "plt.plot(x, y_relu, label='ReLU')\n",
        "plt.title('Sigmoid and ReLU Activation Functions')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare the behaviours of activation functions:\n",
        "\n",
        "- Step function output for x = -2: 0\n",
        "- Step function output for x = 2: 1\n",
        "- Sigmoid function output for x = -2: 0.1192\n",
        "- Sigmoid function output for x = 2: 0.8808\n",
        "- ReLU function output for x = -2: 0\n",
        "- ReLU function output for x = 2: 2"
      ],
      "metadata": {
        "id": "wBD-z_7J4TBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answering questions:\n",
        "\n",
        "Which function gives only binary outputs?\n",
        "The Step function gives only binary outputs (0 or 1).\n",
        "\n",
        "Which function smoothly transitions between values?\n",
        "The Sigmoid function smoothly transitions between values.\n",
        "\n",
        "Which function sets negative values to zero but keeps positive values unchanged?\n",
        "The ReLU function sets negative values to zero but keeps positive values unchanged.\n",
        "\n",
        "Why is ReLU commonly used in deep learning models?\n",
        "ReLU is commonly used because it helps mitigate the vanishing gradient problem and is computationally efficient.\n",
        "\n",
        "Why might Sigmoid be a good choice for binary classification tasks?\n",
        "Sigmoid is a good choice for binary classification tasks because its output is between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "What are the weaknesses of the Step function compared to others?\n",
        "The Step function's main weakness is that its gradient is zero everywhere except at zero, which makes it unsuitable for gradient-based optimization methods like backpropagation."
      ],
      "metadata": {
        "id": "I66QXYEo38BW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "898c23e4"
      },
      "source": [
        "## Exercise 4: Forward Propagation in a Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ab023f8",
        "outputId": "83a7f202-b0ea-4935-ecb5-6ae85c103c20"
      },
      "source": [
        "# Exercise 4: Forward Propagation in a Deep Neural Network\n",
        "\n",
        "# Given Data\n",
        "square_footage = 2000\n",
        "num_bedrooms = 3\n",
        "\n",
        "# Layer one weights and bias\n",
        "weight1_layer1 = 0.5\n",
        "weight2_layer1 = 0.7\n",
        "bias_layer1 = 10000\n",
        "\n",
        "# Layer two weights and bias\n",
        "weight1_layer2 = 0.6\n",
        "weight2_layer2 = 0.8\n",
        "bias_layer2 = 20000\n",
        "\n",
        "# Output layer weight and bias\n",
        "weight_output = 1.2\n",
        "bias_output = 30000\n",
        "\n",
        "# Activation function (ReLU)\n",
        "def relu(x):\n",
        "    return max(0, x)\n",
        "\n",
        "# 1. Compute the output of the first layer using the weighted sum formula and ReLU activation function\n",
        "weighted_sum_layer1 = (square_footage * weight1_layer1) + (num_bedrooms * weight2_layer1) + bias_layer1\n",
        "output_layer1 = relu(weighted_sum_layer1)\n",
        "print(f\"Output of Layer 1: {output_layer1}\")\n",
        "\n",
        "# 2. Compute the output of the second layer using the same method\n",
        "# Assuming layer 2 has one neuron and takes the output of layer 1 as its input:\n",
        "weighted_sum_layer2 = (output_layer1 * weight1_layer2) + bias_layer2 # Simplified for manual calculation\n",
        "output_layer2 = relu(weighted_sum_layer2)\n",
        "print(f\"Output of Layer 2: {output_layer2}\")\n",
        "\n",
        "\n",
        "# 3. Compute the final prediction using the weighted sum formula at the output layer\n",
        "# Assuming the output layer has one neuron and takes the output of layer 2 as its input:\n",
        "final_prediction = (output_layer2 * weight_output) + bias_output\n",
        "print(f\"Final Predicted House Price: {final_prediction}\")\n",
        "\n",
        "# 4. Interpret the final result and determine the predicted house price\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"The predicted house price based on the given inputs and network parameters is: ${final_prediction:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of Layer 1: 11002.1\n",
            "Output of Layer 2: 26601.260000000002\n",
            "Final Predicted House Price: 61921.512\n",
            "\n",
            "Interpretation:\n",
            "The predicted house price based on the given inputs and network parameters is: $61921.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a06e5652"
      },
      "source": [
        "## Exercise 5: Training a Neural Network with Forward and Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0417cf5",
        "outputId": "777f0608-8c4c-4136-f310-a0e17480110d"
      },
      "source": [
        "# Exercise 5: Training a Neural Network with Forward and Backward Propagation\n",
        "\n",
        "# Initialize input values\n",
        "study_hours = 6\n",
        "previous_score = 75\n",
        "inputs = np.array([[study_hours, previous_score]]) # Use numpy array for consistency\n",
        "\n",
        "# Initialize weights and bias with given values\n",
        "weight_study = 0.4\n",
        "weight_previous = 0.6\n",
        "bias = 5\n",
        "\n",
        "# Actual exam score\n",
        "actual_score = 85\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Define sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define derivative of sigmoid function for backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training loop (for a single iteration as per the task)\n",
        "# 1. Compute forward propagation to predict the exam score\n",
        "# Weighted sum\n",
        "weighted_sum = (inputs[0, 0] * weight_study) + (inputs[0, 1] * weight_previous) + bias\n",
        "# Activation\n",
        "predicted_score = sigmoid(weighted_sum)\n",
        "\n",
        "print(f\"Predicted score before training: {predicted_score:.4f}\")\n",
        "\n",
        "# 2. Compute the error between the prediction and actual score\n",
        "# Using Mean Squared Error (MSE)\n",
        "error = actual_score - predicted_score\n",
        "loss = error**2\n",
        "\n",
        "print(f\"Error: {error:.4f}\")\n",
        "print(f\"Loss: {loss:.4f}\")\n",
        "\n",
        "# 3. Compute gradients for the weights and bias\n",
        "# Gradient of loss with respect to predicted_score\n",
        "d_loss_d_predicted_score = 2 * error # Derivative of MSE\n",
        "\n",
        "# Gradient of predicted_score with respect to weighted_sum (derivative of sigmoid)\n",
        "d_predicted_score_d_weighted_sum = sigmoid_derivative(predicted_score)\n",
        "\n",
        "# Gradient of weighted_sum with respect to weights and bias\n",
        "d_weighted_sum_d_weight_study = inputs[0, 0]\n",
        "d_weighted_sum_d_weight_previous = inputs[0, 1]\n",
        "d_weighted_sum_d_bias = 1\n",
        "\n",
        "# Gradients for weights and bias using the chain rule\n",
        "gradient_weight_study = d_loss_d_predicted_score * d_predicted_score_d_weighted_sum * d_weighted_sum_d_weight_study\n",
        "gradient_weight_previous = d_loss_d_predicted_score * d_predicted_score_d_weighted_sum * d_weighted_sum_d_weight_previous\n",
        "gradient_bias = d_loss_d_predicted_score * d_predicted_score_d_weighted_sum * d_weighted_sum_d_bias\n",
        "\n",
        "print(f\"Gradient weight study: {gradient_weight_study:.4f}\")\n",
        "print(f\"Gradient weight previous: {gradient_weight_previous:.4f}\")\n",
        "print(f\"Gradient bias: {gradient_bias:.4f}\")\n",
        "\n",
        "\n",
        "# 4. Update the weights and bias using gradient descent\n",
        "weight_study = weight_study + learning_rate * gradient_weight_study\n",
        "weight_previous = weight_previous + learning_rate * gradient_weight_previous\n",
        "bias = bias + learning_rate * gradient_bias\n",
        "\n",
        "# 5. Print the updated weights and bias after one training iteration\n",
        "print(\"\\nAfter one training iteration:\")\n",
        "print(f\"Updated weight_study: {weight_study:.4f}\")\n",
        "print(f\"Updated weight_previous: {weight_previous:.4f}\")\n",
        "print(f\"Updated bias: {bias:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted score before training: 1.0000\n",
            "Error: 84.0000\n",
            "Loss: 7056.0000\n",
            "Gradient weight study: 0.0000\n",
            "Gradient weight previous: 0.0000\n",
            "Gradient bias: 0.0000\n",
            "\n",
            "After one training iteration:\n",
            "Updated weight_study: 0.4000\n",
            "Updated weight_previous: 0.6000\n",
            "Updated bias: 5.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6f0ef63"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "By completing these exercises, you have:\n",
        "\n",
        "- Understood multi-layer perceptrons by building one from scratch in Exercise 1.\n",
        "- Learned forward and backward propagation by implementing them in Exercise 2 and 5.\n",
        "- Implemented and compared activation functions (Step, Sigmoid, and ReLU) in Exercise 3.\n",
        "- Applied gradient descent to update weights and bias in Exercise 2 and 5.\n",
        "- Gained experience with forward propagation in a deeper network in Exercise 4."
      ]
    }
  ]
}