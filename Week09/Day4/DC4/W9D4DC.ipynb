{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOltebTy5JEPJFVBVaCy6sA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arquansa/PSTB-exercises/blob/main/Week09/Day4/DC4/W9D4DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding Developer Pain\n",
        "Last Updated: March 27th, 2025\n",
        "\n",
        "#Daily Challenge : Decoding Developer Pain: Mapping LLM Challenges from an Empirical Study#\n",
        "\n",
        "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
        "How to interpret and evaluate empirical research methodologies\n",
        "How to analyze the structure and function of a taxonomy in a scientific paper\n",
        "How to extract key insights from large-scale developer studies\n",
        "How to draw actionable implications for LLM development based on evidence\n",
        "How to relate structural paper analysis to real-world software engineering challenges\n",
        "\n",
        "üõ†Ô∏è What you will create\n",
        "A brief analytical essay identifying and evaluating the study‚Äôs methodology and findings\n",
        "A reconstructed taxonomy diagram of LLM developer challenges\n",
        "A table of at least 3 cross-cutting themes between the paper and your experience or expectations as a developer\n",
        "\n",
        "\n",
        "Task\n",
        "paper : An Empirical Study on Challenges for LLM Application Developers\n",
        "\n",
        "1. Read Sections 3 and 6 of the paper (Methodology & Challenge Taxonomy Construction).\n",
        "2. Recreate the taxonomy of challenges (at least 6 inner categories + major subcategories) in a visual diagram or bullet hierarchy.\n",
        "3. In a markdown file, answer the following:\n",
        "\n",
        "What are the key design decisions made in their empirical methodology?\n",
        "How did the authors ensure validity and reliability of their coding procedure?\n",
        "What kinds of challenges dominate LLM development, according to the data?\n",
        "What implications do these challenges have for the design of LLM platforms or APIs?\n",
        "4. Based on your understanding, propose 2 original ideas for tools or community resources that could help solve common developer issues highlighted in the taxonomy.\n",
        "\n"
      ],
      "metadata": {
        "id": "LblK2jQB_E1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#1. Read Sections 3 and 6 of the paper (Methodology & Challenge Taxonomy Construction).\n",
        "#2. Recreate the taxonomy of challenges (at least 6 inner categories + major subcategories) in a visual diagram or bullet hierarchy."
      ],
      "metadata": {
        "id": "EjhPVT4-AF05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Challenges for OpenAI Developers (100%)\n",
        "\n",
        "Fig. 7. Our Constructed Challenge Taxonomy for LLM Developers.\n",
        "\n",
        "[A] General Questions (26.3%)\n",
        "- [A.1] Integration with Custom Applications (17.0%)\n",
        "- [A.2] Conceptional Questions (6.4%)\n",
        "- [A.3] Feature Suggestions (2.9%)\n",
        "\n",
        "[B] API (22.9%)\n",
        "- [B.1] Faults in API (8.7%)\n",
        "- [B.2] Error Messages in API Calling (7.5%)\n",
        "- [B.3] API Usage (6.7%)\n",
        "\n",
        "[C] Generation and Understanding (19.9%)\n",
        "- [C.1] Text Processing (6.8%)\n",
        "- [C.2] Fine-tuning GPT Models (6.7%)\n",
        "- [C.3] Image Processing (2.5%)\n",
        "- [C.4] Embedding Generation (1.8%)\n",
        "- [C.5] Audio Processing (1.4%)\n",
        "- [C.6] Vision Capability (0.7%)\n",
        "\n",
        "[D] Non-functional Properties (15.4%)\n",
        "- [D.1] Cost (3.6%)\n",
        "- [D.2] Rate Limitation (3.2%)\n",
        "- [D.3] Regulation (3.0%)\n",
        "- [D.4] Promotion (2.1%)\n",
        "- [D.5] Token Limitation (2.0%)\n",
        "- [D.6] Security and Privacy (1.5%)\n",
        "\n",
        "[E] GPT Builder (12.1%)\n",
        "- [E.1] Development (11.2%)\n",
        "- [E.2] Testing (0.9%)\n",
        "\n",
        "[F] Prompt (3.4%)\n",
        "- [F.1] Prompt Design (2.3%)\n",
        "- [F.2] Retrieval Augmented Generation (0.4%)\n",
        "- [F.3] Chain of Thought (0.2%)\n",
        "- [F.4] In-context Learning (0.2%)\n",
        "- [F.5] Zero-shot Prompting (0.2%)\n",
        "- [F.6] Tree of Thoughts (0.1%\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m_V-bcZRMFfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. In a markdown file, answer the following:\n",
        "\n",
        "- What are the key design decisions made in their empirical methodology?\n",
        "- How did the authors ensure validity and reliability of their coding procedure?\n",
        "- What kinds of challenges dominate LLM development, according to the data?\n",
        "- What implications do these challenges have for the design of LLM platforms or APIs?\n",
        "\n",
        "**What are the key design decisions made in their empirical methodology?**\n",
        "\n",
        "**Research Methodology Summary**\n",
        "- Data Source Selection (OpenAI Developer Forum, LLM development discussions).\n",
        "- Subforum Filtering (Focused on API, Prompting, GPT Builders, and ChatGPT subforumsn due to their developer-centric content).\n",
        "- Metadata Collection (post-level metadata, e.g., titles, dates, reply counts, to support analysis of engagement and difficulty).\n",
        "- Popularity Trend Analysis (RQ1) (Used time series analysis to track developer engagement over time.\n",
        "- Difficulty Level Assessment (RQ2)\n",
        "- Post Sampling for Manual Annotation\n",
        "- Annotation Procedure\n",
        "- Taxonomy Construction (multi-level, allowing nuanced categorization of developer challenges)\n",
        "- Reliability Analysis (Measured inter-annotator agreement using Cohen‚Äôs Kappa (k = 0.812) to validate consistency).\n",
        "- Handling Ambiguous Posts\n",
        "- Iterative Refinement of Taxonomy, to maintain comprehensiveness.\n",
        "- Extending to Other Platforms\n",
        "\n",
        "Applied the same methodology to GitHub issues to generalize findings beyond the forum."
      ],
      "metadata": {
        "id": "1HnQ8ao9RWfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How did the authors ensure validity and reliability of their coding procedure**?\n",
        "\n",
        "1. Dual Annotation with Expert Coders\n",
        "Two experienced annotators independently labeled the sample of posts.\n",
        "\n",
        "This supports validity, as expert input increases the accuracy and relevance of the coding.\n",
        "\n",
        "2. Conflict Resolution by a Third Arbitrator\n",
        "When disagreements occurred, a third person reviewed and resolved them.\n",
        "\n",
        "This ensured consistent and fair classification of ambiguous cases.\n",
        "\n",
        "3. Use of Open Coding\n",
        "Instead of predefined categories, they used open coding, allowing categories to emerge from the data.\n",
        "\n",
        "This approach helps capture real-world developer challenges, improving content validity.\n",
        "\n",
        "4. Reliability Testing with Cohen‚Äôs Kappa\n",
        "They calculated Cohen‚Äôs Kappa (Œ∫ = 0.812) to measure inter-annotator agreement.\n",
        "\n",
        "A value above 0.8 indicates strong reliability and consistent coding.\n",
        "\n",
        "5. Iterative Refinement of the Taxonomy\n",
        "The taxonomy was updated as new patterns appeared during annotation.\n",
        "\n",
        "This ensured it remained comprehensive and reflective of the actual data.\n",
        "\n",
        "In short, they combined expert input, structured disagreement resolution, statistical validation, and adaptive coding practices to ensure both validity and reliability of the coding process."
      ],
      "metadata": {
        "id": "awlsY3Hl7Vxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What kinds of challenges dominate LLM development, according to the data?**\n",
        "\n",
        "According to the data analyzed from the OpenAI Developer Forum, the dominant challenges in LLM development fall into the following main categories:\n",
        "\n",
        "- Prompt Engineering Issues Crafting effective prompts to get desired outputs\n",
        "Struggling with prompt formatting, length, or clarity\n",
        "- Iterative trial-and-error with prompt tuning\n",
        "- API Usage Problems Errors or confusion with API endpoints, parameters, and rate limits\n",
        "- Integration issues in code (e.g., Python, JavaScript)\n",
        "- Understanding pricing and usage quotas\n",
        "- Model Behavior and Output Quality Unexpected or inconsistent model responses\n",
        "Hallucinations or incorrect factual outputs\n",
        "- Lack of control over tone, style, or content generation\n",
        "- Fine-Tuning and Customization Difficulty training or fine-tuning models effectively\n",
        "- Questions about dataset preparation, token limits, and evaluation\n",
        "- Challenges balancing performance vs. cost\n",
        "- Deployment and Scaling Issues with latency, performance, and reliability in production\n",
        "- Problems managing multi-user access or concurrent sessions\n",
        "- Concerns with cost and system limits at scale\n",
        "- Debugging and Error Handling Trouble interpreting error messages\n",
        "API failures, timeouts, or unexpected crashes\n",
        "- Unclear documentation or lack of examples\n",
        "\n",
        "These challenge types were identified through manual annotation and taxonomy construction, showing that prompting and API integration are especially frequent pain points, followed by model behavior issues and deployment concerns."
      ],
      "metadata": {
        "id": "TbPDPj6K7VhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What implications do these challenges have for the design of LLM platforms or APIs**?\n",
        "\n",
        "The challenges developers face with LLMs have direct implications for how LLM platforms and APIs should be designed. Here are the key takeaways:\n",
        "\n",
        "1. Simplify and Support Prompt Engineering\n",
        "Implication:\n",
        "LLM platforms should offer better tools for writing and testing prompts.\n",
        "\n",
        "Design Suggestions:\n",
        "\n",
        "Provide interactive prompt playgrounds with real-time feedback\n",
        "\n",
        "Include prompt templates and best-practice libraries\n",
        "\n",
        "Offer explanations for model behavior to help with tuning\n",
        "\n",
        "2. Improve API Usability and Documentation\n",
        "Implication:\n",
        "Confusion with API usage shows the need for clearer, developer-friendly design.\n",
        "\n",
        "Design Suggestions:\n",
        "\n",
        "Provide simplified, code-ready examples in multiple languages\n",
        "\n",
        "Add better error messages and debugging tips\n",
        "\n",
        "Include interactive documentation or API explorers\n",
        "\n",
        "3. Enhance Transparency and Control Over Model Output\n",
        "Implication:\n",
        "Developers need more control and predictability in model responses.\n",
        "\n",
        "Design Suggestions:\n",
        "\n",
        "Offer parameter explanations with visualizations (e.g., temperature, top_p)\n",
        "\n",
        "Introduce tools to evaluate output quality or detect hallucinations\n",
        "\n",
        "Enable content filters or guidelines to shape tone and behavior\n",
        "\n",
        "4. Streamline Fine-Tuning and Customization\n",
        "Implication:\n",
        "Customizing models remains too complex for many users.\n",
        "\n",
        "Design Suggestions:\n",
        "\n",
        "Build no-code or low-code fine-tuning interfaces\n",
        "\n",
        "Provide clear walkthroughs for dataset preparation\n",
        "\n",
        "Offer evaluation metrics and validation tools\n",
        "\n",
        "5. Support Scalable and Reliable Deployment\n",
        "Implication:\n",
        "Deployment concerns reflect the need for more robust infrastructure support.\n",
        "\n",
        "Design Suggestions:\n",
        "\n",
        "Offer pre-built deployment kits (e.g., Docker, cloud integrations)\n",
        "\n",
        "Provide auto-scaling support and usage alerts\n",
        "\n",
        "Improve monitoring tools for latency, costs, and load\n",
        "\n",
        "6. Better Error Handling and Debugging Support\n",
        "Implication:\n",
        "Difficulties with error messages suggest the need for better developer experience.\n",
        "\n",
        "Design Suggestions:\n",
        "\n",
        "Make error messages more descriptive and actionable\n",
        "\n",
        "Offer diagnostic tools to trace issues (e.g., input, context window overflow)\n",
        "\n",
        "Integrate community-sourced troubleshooting tips\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QkfdR3dG7U-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Based on your understanding, propose 2 original ideas for tools or community resources that could help solve common developer issues highlighted in the taxonomy.**\n",
        "\n",
        "  1. LLM platforms should shift from being raw model APIs to developer-centered tools\n",
        "  2. Tools used by developer should integrate built-in guidance, transparency, and facilitator features that reduce the need for trial-and-error and technical guesswork."
      ],
      "metadata": {
        "id": "V5hhE__4RXRL"
      }
    }
  ]
}