{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arquansa/PSTB-exercises/blob/main/Week09/Day2/DC2/W9D2DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Daily Challenge : Run Code Llama on Kaggle using Gemma#\n",
        "\n",
        "Last Updated: April 29th, 2025\n",
        "\n",
        "👩‍🏫 👩🏿‍🏫 What You’ll learn\n",
        "How to run open-source LLMs like Code Llama on Kaggle using llama.cpp\n",
        "How to install and configure Gemma with CUDA support\n",
        "How to convert Hugging Face models\n",
        "How to generate code and text using Code Llama\n",
        "\n",
        "🛠️ What you will create\n",
        "You’ll deploy Code Llama 7B on Kaggle and generate Python code through prompts. You’ll also explore small exercises to play with inference using your own ideas!\n",
        "\n",
        "Warning : The Code Llama 7B model is not being publicly accessible on Kaggle without an approved access request.\n",
        "To do this exercise you will first need to request for access (which should not take too long) here\n",
        "\n",
        "In case you don’t get approved, you can consider the following alternative to avoid access requests : you will opt for a smaller and publicly available language model llama2-7b-chat that doesn’t require access requests. Download the Code Llama 7B model weights from Hugging Face and upload them to your Kaggle working directory or a publicly accessible storage location like Google Drive. You can then load the model from the downloaded weights.\n",
        "\n",
        "!pip install llama-cpp-python --verbose\n",
        "\n",
        "!wget https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q8_0.gguf -O gemma-3-1b-it-Q8_0.gguf # Gemma 3 1B model Q8 in GGUF format (compatible with Llama)\n",
        "\n",
        "🛠️ What you will use?\n",
        "The method: using llama.cpp and llama-cpp-python for efficient inference\n",
        "The models: Code Llama 7B (or llama2-7b-chat as fallback) and Gemma\n",
        "The tools: Hugging Face, GGUF conversion, CUDA bindings\n",
        "\n",
        "\n",
        "💼 Prerequisites\n",
        "Kaggle account with GPU enabled\n",
        "(Optional) Access approved for Code Llama 7B\n",
        "Python basics and comfort with command line\n",
        "\n",
        "#Task\n",
        "\n",
        "**Part 1 : Install llama-cpp-python with CUDA bindings and clone the llama.cpp GitHub repository.**\n",
        "\n",
        "\n",
        "**Part 2 : Model Preparation : Convert the Hugging Face Code Llama model into GGUF format (optimized for llama.cpp).**\n",
        "\n",
        "\n",
        "**Part 3 : Load the converted model using the llama-cpp-python bindings.**\n"
      ],
      "metadata": {
        "id": "TyjJIbdPXlL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 1 : Install llama-cpp-python with CUDA bindings and clone the llama.cpp GitHub repository.**\n",
        "\n",
        "1. Create a new Kaggle or Google Colaboratory notebook with GPU runtime.\n",
        "\n",
        "2. Install dependencies:\n",
        "\n",
        "!pip install llama-cpp-python --verbose\n",
        "3. Clone llama.cpp:\n",
        "\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "\n",
        "\n",
        "What you’ll use:\n",
        "\n",
        "Kaggle or Colab notebook UI\n",
        "pip for package installation\n",
        "git for repository cloning"
      ],
      "metadata": {
        "id": "1hVoFkr8atXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Create a new Kaggle or Google Colaboratory notebook with GPU runtime.**"
      ],
      "metadata": {
        "id": "MFeyDSk4m6s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*# Fait - le notebook colab en cours*"
      ],
      "metadata": {
        "id": "VDTvnNYh7_HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Install dependencies:**"
      ],
      "metadata": {
        "id": "v3hz7GdgfQeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python --verbose"
      ],
      "metadata": {
        "id": "YPfKbiTVbsCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Clone llama.cpp:**"
      ],
      "metadata": {
        "id": "PQAAn2yPe-S1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq4jrjqEXjJQ",
        "outputId": "eaee40fe-988f-47a7-ec1d-4b5ab67a5a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 58076, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (175/175), done.\u001b[K\n",
            "remote: Total 58076 (delta 140), reused 46 (delta 46), pack-reused 57855 (from 4)\u001b[K\n",
            "Receiving objects: 100% (58076/58076), 137.96 MiB | 11.77 MiB/s, done.\n",
            "Resolving deltas: 100% (41990/41990), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2 : Model Preparation : Convert the Hugging Face Code Llama model into GGUF format (optimized for llama.cpp).**\n",
        "\n",
        "**Option A:** Code Llama 7B (if approved)\n",
        "\n",
        "Request and download the model via Kaggle UI.\n",
        "\n",
        "Convert to GGUF for llama.cpp:\n",
        "\n",
        "cd llama.cpp python3 convert-hf-to-gguf.py\n",
        "--model hf/metaresearch/codellama-7b-pytorch\n",
        "--outfile codellama-7b.gguf\n",
        "\n",
        "**Option B:**\n",
        "1. llama2-7b-chat (public fallback)\n",
        "2. Download weights from Hugging Face:\n",
        "!wget https://huggingface.co/llama2/7b-chat/resolve/main/llama2-7b-chat.gguf\n"
      ],
      "metadata": {
        "id": "4PCp4pRjgP7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==> **B option was chosen and implemented**"
      ],
      "metadata": {
        "id": "qKIcT3GI9vMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Request and download the model via Kaggle UI - llama2-7b-chat (public fallback).**"
      ],
      "metadata": {
        "id": "Yes4Opcy-33y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-stack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRoabOLL7K6M",
        "outputId": "e6ac0c55-c4e7-42aa-89f8-2593232206b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-stack in /usr/local/lib/python3.11/dist-packages (0.2.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from llama-stack) (3.12.14)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.116.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.34.1)\n",
            "Requirement already satisfied: jinja2>=3.1.6 in /usr/local/lib/python3.11/dist-packages (from llama-stack) (3.1.6)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from llama-stack) (4.25.0)\n",
            "Requirement already satisfied: llama-stack-client>=0.2.12 in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.2.12)\n",
            "Requirement already satisfied: openai>=1.66 in /usr/local/lib/python3.11/dist-packages (from llama-stack) (1.97.1)\n",
            "Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.11/dist-packages (from llama-stack) (3.0.51)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from llama-stack) (1.1.1)\n",
            "Requirement already satisfied: python-jose in /usr/local/lib/python3.11/dist-packages (from llama-stack) (3.5.0)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from llama-stack) (2.11.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from llama-stack) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from llama-stack) (13.9.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from llama-stack) (75.2.0)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.47.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from llama-stack) (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from llama-stack) (11.3.0)\n",
            "Requirement already satisfied: h11>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.16.0)\n",
            "Requirement already satisfied: python-multipart>=0.0.20 in /usr/local/lib/python3.11/dist-packages (from llama-stack) (0.0.20)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1.0,>=0.115.0->llama-stack) (4.14.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.6->llama-stack) (3.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-stack-client>=0.2.12->llama-stack) (4.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from llama-stack-client>=0.2.12->llama-stack) (8.2.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-stack-client>=0.2.12->llama-stack) (1.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-stack-client>=0.2.12->llama-stack) (2.2.2)\n",
            "Requirement already satisfied: pyaml in /usr/local/lib/python3.11/dist-packages (from llama-stack-client>=0.2.12->llama-stack) (25.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from llama-stack-client>=0.2.12->llama-stack) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from llama-stack-client>=0.2.12->llama-stack) (4.67.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-stack) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-stack) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-stack) (3.10)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66->llama-stack) (0.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->llama-stack) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->llama-stack) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->llama-stack) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->llama-stack) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->llama-stack) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->llama-stack) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->llama-stack) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->llama-stack) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->llama-stack) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->llama-stack) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->llama-stack) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->llama-stack) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->llama-stack) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->llama-stack) (1.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->llama-stack) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->llama-stack) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->llama-stack) (0.26.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
            "Requirement already satisfied: ecdsa!=0.15 in /usr/local/lib/python3.11/dist-packages (from python-jose->llama-stack) (0.19.1)\n",
            "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from python-jose->llama-stack) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from python-jose->llama-stack) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->llama-stack) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->llama-stack) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->llama-stack) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->llama-stack) (2.19.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->llama-stack) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from ecdsa!=0.15->python-jose->llama-stack) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llama model list --show-all # A list of models to be selected in the Hugging Face Repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oDXrrV-7T1T",
        "outputId": "4fad96c1-ca82-48ad-c9b0-2403b83a5544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mModel Descriptor(ID)        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mHugging Face Repo           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mContext Length\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-7b                  \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-7b       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-13b                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-13b      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-70b                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-70b      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-7b-chat             \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-7b-chat  \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-13b-chat            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-13b-chat \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-70b-chat            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-70b-chat \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-8B                  \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-8B       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-70B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-70B      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-8B-Instruct         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-8B-Instr…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-70B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-70B-Inst…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-8B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-8B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-70B                \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-70B    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B:bf16-mp8      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B               \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-F…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B:bf16-mp16     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-8B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-8B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-70B-Instruct       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-70B-In…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct:bf16…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct:bf16…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-11B-Vision         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-11B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-90B-Vision         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-90B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct:int4-q…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct:int4-s…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct:int4-q…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct:int4-s…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-11B-Vision-Instruct\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-11B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-90B-Vision-Instruct\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-90B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.3-70B-Instruct       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.3-70B-In…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Scout-17B-16E       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Scout-17…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m256K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m256K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Scout-17B-16E-Instr…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Scout-17…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m10240K        \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m1024K         \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m1024K         \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-4-12B           \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-4-12B\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-11B-Vision    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-11…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-1B:int4       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-1B…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-1B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-1B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-8B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-8B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-8B:int8       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-8B…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-2-8B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-2-8B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mPrompt-Guard-86M            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Prompt-Guard-86M \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m0K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Prompt-Guard-2-86M    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Prompt-Gua…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m0K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
            "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Prompt-Guard-2-22M    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Prompt-Gua…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m0K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
            "└──────────────────────────────┴──────────────────────────────┴────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llama download --source meta --model-id Llama-2-7b # Model chosen and uploaded from Hugging Face Repo"
      ],
      "metadata": {
        "id": "qMpMwFWN9pJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llama model verify-download --model-id Llama-2-7b ## Vérification de l'installation du modèle choisi depuis Hugging Face Repo#/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXJkjJaxHWSy",
        "outputId": "a546776f-014e-4ecc-d47c-d81f75a12102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K\n",
            "\u001b[?25h\n",
            "Verification Results:\n",
            "\u001b[32m✓ consolidated.\u001b[0m\u001b[1;32m00.\u001b[0m\u001b[32mpth: Verified\u001b[0m\n",
            "\u001b[32m✓ params.json: Verified\u001b[0m\n",
            "\n",
            "\u001b[32mAll files verified successfully!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading has been checked and was successfully implemented. All files were successfully checked."
      ],
      "metadata": {
        "id": "2UYwCB2P5Gob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Download weights from Hugging Face (conversion of the Hugging Face Code Llama model into GGUF format - optimized for llama.cpp)**"
      ],
      "metadata": {
        "id": "eG3k8ChY_kJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxTBc7taNN8J",
        "outputId": "ef94c9bc-c2a7-4dba-c461-d4a78fca4d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-02 14:35:01--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.61, 3.165.160.59, 3.165.160.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/08a5566d61d7cb6b420c3e4387a39e0078e1f2fe5f055f3a03887385304d4bfa?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-7b-chat.Q4_K_M.gguf%22%3B&Expires=1754148635&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDE0ODYzNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzA4YTU1NjZkNjFkN2NiNmI0MjBjM2U0Mzg3YTM5ZTAwNzhlMWYyZmU1ZjA1NWYzYTAzODg3Mzg1MzA0ZDRiZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=WsetnLann%7EQl9gizG-A9iqSCXT%7Eap59arHfHHrbQhMUq0j-Rkk9xWPmKuo7P2BPcfnabLMWpVomgoqCxpl6dRQYuVOjSbrIvWuB89E%7E7CmQ%7Eknkxy1VNtukHxKkNOKUd9zYxSmBvICfUFg30Ozi0urUx19QDf3NnKaHqrRzmD81dXxasCo%7EGo9tUYiS58c5ScMpnigozRHi%7ELYHF60P3DkeQENo9ebmJkg0QnFtF6yoNJFzbusUzhmZqxGHb-YYwdxmXlWymw1MkZ9kHGc%7EBOQ-xjc9guj4LahPH7hAdeI3hebZIr2qTbFGibjaz477hqkK97dl9IA9RlXAGv-T6mQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-08-02 14:35:02--  https://cdn-lfs.hf.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/08a5566d61d7cb6b420c3e4387a39e0078e1f2fe5f055f3a03887385304d4bfa?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-7b-chat.Q4_K_M.gguf%22%3B&Expires=1754148635&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDE0ODYzNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzA4YTU1NjZkNjFkN2NiNmI0MjBjM2U0Mzg3YTM5ZTAwNzhlMWYyZmU1ZjA1NWYzYTAzODg3Mzg1MzA0ZDRiZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=WsetnLann%7EQl9gizG-A9iqSCXT%7Eap59arHfHHrbQhMUq0j-Rkk9xWPmKuo7P2BPcfnabLMWpVomgoqCxpl6dRQYuVOjSbrIvWuB89E%7E7CmQ%7Eknkxy1VNtukHxKkNOKUd9zYxSmBvICfUFg30Ozi0urUx19QDf3NnKaHqrRzmD81dXxasCo%7EGo9tUYiS58c5ScMpnigozRHi%7ELYHF60P3DkeQENo9ebmJkg0QnFtF6yoNJFzbusUzhmZqxGHb-YYwdxmXlWymw1MkZ9kHGc%7EBOQ-xjc9guj4LahPH7hAdeI3hebZIr2qTbFGibjaz477hqkK97dl9IA9RlXAGv-T6mQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.172.170.108, 18.172.170.5, 18.172.170.29, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.172.170.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4081004224 (3.8G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.Q4_K_M.gguf’\n",
            "\n",
            "llama-2-7b-chat.Q4_ 100%[===================>]   3.80G  51.1MB/s    in 39s     \n",
            "\n",
            "2025-08-02 14:35:40 (101 MB/s) - ‘llama-2-7b-chat.Q4_K_M.gguf’ saved [4081004224/4081004224]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weights have been uploaded from HuggingFace in order to convert the HuggingFace code Llama into GGUF format, optimized for Llama.cpp."
      ],
      "metadata": {
        "id": "J90ntL9U5i7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 3 : Load the converted model using the llama-cpp-python bindings.**\n",
        "\n",
        "1. Initialize the Python environment:\n",
        "\n",
        "from llama_cpp import Llama\n",
        "2. Load the GGUF model:\n",
        "\n",
        "llm = Llama(\n",
        "   model_path=\"...\",\n",
        "   n_gpu_layers=..,       # adjust for GPU memory\n",
        "   n_threads=...\n",
        ")\n",
        "3. Generate text using a natural language prompt (e.g., ask about the solar system)\n",
        "\n",
        "prompt = \"Explain how the solar system formed.\"\n",
        "output = llm(prompt=..., max_tokens=...)\n",
        "print(output[\"choices\"][0][\"text\"])\n",
        "4. Generate Python code by prompting the model to write a script (e.g., loading a model with Hugging Face)\n",
        "\n",
        "output_stream = llm(\n",
        "   prompt=\"Write a Python script that loads a Hugging Face model and tokenizes input.\",\n",
        "   max_tokens=...,\n",
        "   stream=True\n",
        ")\n",
        "for token_info in output_stream:\n",
        "   print(token_info[\"...\"], end=\"\", flush=True)\n",
        "\n",
        "\n",
        "What you’ll use:\n",
        "\n",
        "llama_cpp.Llama bindings\n",
        "Model path and hardware configuration\n",
        "Prompt engineering fundamentals\n",
        "Streaming inference API\n",
        "Hint : (complete code to fill where there is “…”)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = ...\n",
        "\n",
        "prompt = ...\n",
        "\n",
        "output = ...\n",
        "print(output[\"choices\"][0][\"text\"])\n",
        "\n",
        "output_stream = llm(\n",
        "    ...,\n",
        "    max_tokens=...,\n",
        "    stream=True  # Enable streaming\n",
        ")\n",
        "\n",
        "Iterate through the tokens and print them at the same time they are computed\n",
        "\n",
        "for output in output_stream:\n",
        "    token = ...\n",
        "    print(token, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "yojEVq9ynjsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Initialize the Python environment:**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cPxJYT1PPjfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "6te4WDIqQM-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python environment initialized."
      ],
      "metadata": {
        "id": "XLMFaCxl7Fzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Load the GGUF model:**"
      ],
      "metadata": {
        "id": "urvDMPoKPjQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "    model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",  # Replace with the actual path to your GGUF model file\n",
        "    n_gpu_layers=0,  # Adjust for GPU memory. Set to 0 to use CPU.\n",
        "    n_threads=None   # Use default number of threads\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djlu5rhLQOZw",
        "outputId": "5ef08172-3ea0-4fe6-9130-49fb56d5a470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 6.74 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =  2943.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "......................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n",
            "llama_kv_cache_unified: size =  256.00 MiB (   512 cells,  32 layers,  1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =    89.51 MiB\n",
            "llama_context: graph nodes  = 1158\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GGUF model uploaded."
      ],
      "metadata": {
        "id": "nQO650VJ8LG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Generate text using a natural language prompt (e.g., ask about the solar system)**"
      ],
      "metadata": {
        "id": "MzxO_22HPioY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain how the solar system formed.\"\n",
        "output = llm(prompt, max_tokens=32)\n",
        "print(output[\"choices\"][0][\"text\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-9YzztRQPDo",
        "outputId": "d5054f64-84a4-4e92-ab29-a26857e1bbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    5214.04 ms\n",
            "llama_perf_context_print: prompt eval time =    2581.44 ms /     8 tokens (  322.68 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =   20271.29 ms /    31 runs   (  653.91 ms per token,     1.53 tokens per second)\n",
            "llama_perf_context_print:       total time =   22870.43 ms /    39 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " nobody really knows how the solar system formed. but there are several theories that scientists have developed over the years. one of the most widely accepted theories is the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generated using a natural language prompt '\"Explain how the solar system formed.\"'."
      ],
      "metadata": {
        "id": "rzMANtnZ8ThT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Generate Python code by prompting the model (e.g.,\"Write a Python script that loads a Hugging Face model and tokenizes input.\")**"
      ],
      "metadata": {
        "id": "a6WSeI4HPiXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_stream = llm(\n",
        "   prompt=\"Write a Python script that loads a Hugging Face model and tokenizes input.\",\n",
        "    max_tokens=256, # Increased max_tokens for a more complete script\n",
        "    stream=True  # Enable streaming\n",
        ")\n",
        "\n",
        "# Iterate through the tokens and print them at the same time they are computed\n",
        "for token_info in output_stream:\n",
        "    token = token_info[\"choices\"][0][\"text\"]\n",
        "    print(token, sep = \"/n\", end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDARGfA6QPzY",
        "outputId": "58081c74-b871-403b-df5b-5e2eec3384f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 17 prefix-match hit, remaining 1 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " surely, you can use the `from_pretrained` method to load a model. Here's an example of how you can use it to load a BERT model:\n",
            "```\n",
            "import torch\n",
            "from transformers import BertTokenizer, BertModel\n",
            "\n",
            "# Load the BERT model\n",
            "model = BertModel.from_pretrained('bert-base-uncased')\n",
            "\n",
            "# Load the tokenizer\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
            "\n",
            "# Test the tokenizer\n",
            "text = \"This is a sample input text.\"\n",
            "input_ids = tokenizer.encode_plus(text, \n",
            "```\n",
            "In this example, we first load the BERT model using the `from_pretrained` method. This method takes the name of the model checkpoint as an argument and returns a `BertModel` instance.\n",
            "Next, we load the tokenizer using the `from_pretrained` method. This method takes the name of the tokenizer checkpoint as an argument and returns a `BertTokenizer` instance.\n",
            "Finally, we test the tokenizer by calling the `encode_plus` method on it and passing"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    5214.04 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =  166546.93 ms /   256 runs   (  650.57 ms per token,     1.54 tokens per second)\n",
            "llama_perf_context_print:       total time =  167083.62 ms /   257 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python code generated by prompting the model: prompt=\"Write a Python script that loads a Hugging Face model and tokenizes input.\""
      ],
      "metadata": {
        "id": "ZWHEaz7q84RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus Exercises 🎯**\n",
        "\n",
        "Try these fun and simple prompts to test Code Llama’s coding ability:\n",
        "\n",
        "**I tested bonus exercises 1 & 2.**\n",
        "\n",
        "1. Ask Code Llama to generate a function that checks if a number is prime.\n",
        "\n",
        "2. Prompt it to create a script that reads a CSV file and plots a line chart using matplotlib."
      ],
      "metadata": {
        "id": "Yc9kRappf7jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Ask Code Llama to generate a function that checks if a number is prime.**"
      ],
      "metadata": {
        "id": "KFEfvlydAaCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Génération du prompt via llama\n",
        "prompt = \"Ask Code Llama to generate a function that checks if a number is prime.\"\n",
        "output = llm(prompt, max_tokens=256)\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFVEd6S8gAfv",
        "outputId": "9da48228-f316-4137-ce8f-3884a3724d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 17 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    5214.04 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =  167734.13 ms /   256 runs   (  655.21 ms per token,     1.53 tokens per second)\n",
            "llama_perf_context_print:       total time =  167927.88 ms /   257 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I am glad you asked! Here is a function that checks if a number is prime:\n",
            "```\n",
            "def is_prime(n):\n",
            "    if n <= 1 or n % 2 == 0:\n",
            "        return False\n",
            "    for i in range(3, int(n ** 0.5) + 1, 2):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "```\n",
            "This function uses the Sieve of Eratosthenes algorithm to check if a number is prime. The function starts by checking if the number is less than or equal to 1, or if it is even. If it is, then it is not prime, so the function returns `False`.\n",
            "If the number is greater than 1 and even, the function then checks if it is divisible by any prime number less than or equal to the square root of the number. If it is, then it is not prime, so the function returns `False`.\n",
            "If the number is greater than 1 and not even, then it is prime, so the function returns `True`.\n",
            "Here is an example of how you can use this function:\n",
            "```\n",
            "n = 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vérification du prompt généré par llama\n",
        "# Résultat : Code généré OK\n",
        "def is_prime(n):\n",
        "    if n <= 1 or n % 2 == 0:\n",
        "        return False\n",
        "    for i in range(3, int(n ** 0.5) + 1, 2):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "n = 23\n",
        "print(is_prime(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8ZN5JDcjFhO",
        "outputId": "63cab22c-7e46-4f6b-fb58-8f158f0803e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama's ability to code tested by asking code Llama if a number is prime.\n",
        "\n",
        "Code has been generated correctly."
      ],
      "metadata": {
        "id": "gQ80npB89xjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Prompt Llama to create a script that reads a CSV file and plots a line chart using matplotlib.**"
      ],
      "metadata": {
        "id": "74ST-D_sA7Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Génération du code via llama\n",
        "prompt = \"Create a script that reads a CSV file and plots a line chart using matplotlib\"\n",
        "output = llm(prompt, max_tokens=512)\n",
        "print(output[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSfTBv3lkYR3",
        "outputId": "80379c50-948f-4794-8262-b5c617571545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 15 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    5214.04 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =  226026.40 ms /   354 runs   (  638.49 ms per token,     1.57 tokens per second)\n",
            "llama_perf_context_print:       total time =  226331.30 ms /   355 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "import csv\n",
            "\n",
            "# Load the CSV file\n",
            "with open('data.csv', 'r') as f:\n",
            "    reader = csv.DictReader(f)\n",
            "\n",
            "# Plot the data\n",
            "plt.plot(reader['date'], reader['value'])\n",
            "\n",
            "# Add axis labels\n",
            "plt.xlabel('Date')\n",
            "plt.ylabel('Value')\n",
            "\n",
            "# Show the plot\n",
            "plt.show()\n",
            "\n",
            "This script will read a CSV file named \"data.csv\" and plot a line chart of the data in the file. The script uses the `csv` module to read the file and the `matplotlib.pyplot` module to create the plot.\n",
            "The script first loads the CSV file using the `open()` function in read mode (`'r'`). The `csv.DictReader()` function is then used to read the file and create a dictionary of the data in the file, where the keys are the column names and the values are the data values.\n",
            "The script then uses the `plot()` function to plot the data, passing in the column names as the x-axis and y-axis values. The `xlabel()` and `ylabel()` functions are used to add axis labels to the plot.\n",
            "Finally, the script uses the `show()` function to display the plot.\n",
            "You will need to replace \"data.csv\" with the name of your CSV file.\n",
            "Note: This script assumes that the CSV file has two columns, with the first column named \"date\" and the second column named \"value\". If your CSV file has a different structure, you will need to modify the script accordingly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vérification du prompt généré par llama\n",
        "# Résultat ==> le code renvoie une erreur : 'DictReader' object is not subscriptable**\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Load the CSV file\n",
        "with open('data.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(reader['date'], reader['value'])\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7gUxC-tF0pg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vérification du code généré par Llama (OK après corrections mineures)\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Load the CSV file\n",
        "dates = [] #correction : création d'une liste 'dates'\n",
        "values = [] #correction : création d'une liste 'values'\n",
        "with open('data.csv', 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader: #Correction : transfert du dictionnaire dans les deux listes\n",
        "        dates.append(row['date'])\n",
        "        values.append(row['value'])\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(dates, values)\n",
        "\n",
        "# Add axis labels\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "zaEpAil03MJK",
        "outputId": "eca25eab-1a8f-4882-8476-19e14eda1d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama was prompted to load a .csv file (the one I had to create) and plot a line chart.\n",
        "\n",
        "Code was generated using my prompt, but generated code did not allow me to create the plot, so I had to slightly modify the code to be able to get a plot: Lama generated a dictionary that could not be directly plotted.\n",
        "\n",
        "I had to turn the dictionary into a list to get a plot.  "
      ],
      "metadata": {
        "id": "fvmgzmf6-3UW"
      }
    }
  ]
}